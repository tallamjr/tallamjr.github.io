{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"About Me","text":"<p>Hello! my name is Tarek</p> <p>I am a Machine Learning Research Engineer and Technical Lead for Efficient Machine Learning and Intelligent Embedded Systems at the Alan Turing Institute (ATI) \u23af the United Kingdom's national institute for data science and artificial intelligence, founded in 2015 and largely funded by the UK government. It is named after Alan Turing, the British mathematician and computing pioneer.</p> <p>In my role at the ATI, I work on applied research projects largely relating to topics in Compression  and AI Compiler research to better enable low-latency energy efficient inference of machine learning models in resource constrained Embedded Systems  -- a.k.a #tinyML</p> <p>In a past-life I did my PhD in Applied Machine Learning at the Centre Data Intensive Science &amp; Industry at University College London (UCL). My PhD research focused on the application of Deep Learning and Model Compression algorithms for real-time classification of Astronomical transient events.</p> <p>My thesis, for which I won the Perren prize for best CDT thesis 2022, can be freely downloaded from the UCL archives here.</p> <p>Towards the end of my PhD I was one of the 7 research students that were nominated by UCL to apply for the Schmidt Science Fellowship. My research proposal on \"Efficient Learned Image Reconstruction and Compression Algorithms for Real-time Medical Image Analysis\" is a good reflection of some of the areas of research I find most interesting.</p> <p>I am also a Research Software Engineering Fellow at the Software Sustainability Institute, where I help academics leverage tools and software engineering best practises for better research software #bettersoftwarebetterscience</p> <p>Find me online <code>@tallamjr</code> \\(\\in \\{\\) ,  ,  \\(\\}\\)</p>"},{"location":"allamlabs/","title":"Allam Labs.","text":"<p>Below is a collection of open-source personal projects I am working on, as well as some very interesting projects I am lucky enough to be involved with. If you like the work here and would like to support more from me, you can help sponsor contributions here </p>"},{"location":"allamlabs/#simurgh","title":"simurgh","text":"<p><code>simurgh</code> (pronounced Seymour) is an open source platform that supports developing and evaluating algorithms (AI agents) for automated air traffic control. It provides an easy to use interface for running experiments in an air traffic simulator as well as packages that support agent development.</p> <p>Air traffic control (ATC) is a complex task requiring real-time safety-critical decision making. In practice, air traffic control operators (ATCOs) monitor a given sector and issue commands to aircraft pilots to ensure safe separation between aircraft. They also have to consider the number and frequency of instructions issued, fuel efficiency and orderly handover between sectors. Optimising for the multiple objectives while accounting for uncertainty (e.g., due to aircraft mass, pilot behaviour or weather conditions) makes this a particularly complex task.</p> <p>The Simurgh project provides a research-focused user-friendly platform for testing automated approaches to ATC</p>"},{"location":"allamlabs/#wwwastroinformaticsxyz","title":"www.astroinformatics.xyz","text":"<p><code>astroinformatics.xyz</code> is a online book inspired by the OpenAI book SpinningUp. The idea is to have a resource that outlines the mathematics and technology involved with doing Astroinformatics and the Mathematics and Data Science that underpins the research. It is still very much a work in progress, but the plan is to have 3 main sections: 1. Data Science for the Mathematics and Machine Learning theory. 2. Data Engineering; to showcase the technologies used, like Apache Spark or Apache Kafka for instance. 3. Research Engineering to outline the tools of the trade for reproducible research, model deployment and production system design.</p>"},{"location":"allamlabs/#option3","title":"option3","text":"<p><code>option3</code> is a proof of concept application that is being used to improve my understanding of Kafka and Spark for developing machine learning data pipelines. Inspired by Stephane Maarek's Kafka for Beginners course, I hope to connect to the Twitter stream of tweets, apply some filtering and transformations, and finally visualise, in real-time, the processed data.</p> <p></p> <p>Follow me on Github and check out more of my projects.</p> <p> </p> <p></p>"},{"location":"blog/","title":"Tarek's Tech Blog","text":""},{"location":"blog/2016/11/14/-multilingual-notebooks-a-polyglots-playground/","title":"\ud83d\udcca Multilingual Notebooks: A Polyglot's Playground","text":"<p>A walk through of the steps installing MATLAB, R and Julia kernels for Jupyter notebooks, with some gotchas you may encounter along the way.</p> <p></p> <p>Jupyter notebooks are an amazing way of keeping track of programming pipelines and for learning concepts through programming. It is also a great way to recall how you may have done something before, or solved a problem previously (bishop2006pattern?)</p> <p>I have been using Jupyter notebooks (previously know as the IPython notebook) for a while but only for use with Python 2.7 programming language. Following a brilliant Coursera course on Julia Scientific Programming, I discovered that there has been a push to make these notebooks agnostic to language. There are several kernels available at the moment for a multitude of languages including Julia, R, Ruby and so on, a full list of which can be found here. This post will talk though the steps involved to install the MATLAB, R and Julia kernels.</p> <ul> <li>MATLAB</li> <li>R</li> <li>Julia</li> </ul>","tags":["python","matlab","notebooks"]},{"location":"blog/2016/11/14/-multilingual-notebooks-a-polyglots-playground/#matlab","title":"MATLAB","text":"<p>MATLAB (matrix laboratory) is ubiquitous in the world of scientific computing and is a many engineering and physics students only connection to programming. The brilliant abstraction away from the low-level implementation of LAPACK under the hood with a nice high level interface make MATLAB a very powerful and useful tool. Combine that with the Jupyter notebook and we now have a way of expressing complex mathematical concepts and doing compute numerical methods in an easy to read manner.</p> <p>To get started one needs to have MATLAB installed on their local machine. MATLAB is propriety software from the mathworks company but if you are affiliated with a university a license should be easy to come by. After the binary is installed, one needs to install the MATLAB engine for Python, instructions for downloading can be found here. The system requirements for these installs can be found here, however, on the system requirement website, it states Python 3.5 is supported, but I found this not to be the case (14-11-2016) as I encountered the following error under a 3.5 environment:</p> <pre><code>OSError: MATLAB Engine for Python supports Python version 2.7, 3.3 and 3.4, \\\nbut your version of Python is 3.5\n</code></pre> <p>Therefore, it was decided to create a new environment to play it safe under Python 3.4 using conda like so;</p> <pre><code>conda create -n py34 python=3.4 anaconda\nsource activate p34\n</code></pre> <p>Then, open up MATLAB and type in:</p> <pre><code>cd \"matlabroot/extern/engines/python\"\npython setup.py install\n</code></pre> <p>This will install the MATLAB engine for Python. Following that we need to ensure a few dependencies are also installed.</p> <pre><code>pip install --upgrade pip\npip install jupyter\npip install pymatbridge\npip install matlab_kernel\npython -m matlab_kernel install\n</code></pre> <p>One may encounter the following error, this can be resolved by doing the following:</p> <pre><code>conda remove pyzmq &amp;&amp; pip install pyzmq\n</code></pre> <p>Once all of these steps have been completed, you should be able to start a new Jupyter notebook with a MATLAB kernel like so:</p> <pre><code>jupyter notebook\n</code></pre> <p>Then in the notebook, select MATLAB from the 'New' menu in the top right hand corner. Alternatively, from the command line, one can simply run:</p> <pre><code>jupyter console --kernel matlab\n</code></pre> <p></p> <p>MATLAB kernel</p>","tags":["python","matlab","notebooks"]},{"location":"blog/2016/11/14/-multilingual-notebooks-a-polyglots-playground/#r","title":"R","text":"<p>R, the successor to S, is a statical programming language. Unlike MATLAB, R is an open source language that has now been around for over 20 years. Over that time, it established itself as the language of choice for mathematical statistician and is widely used today. Although slower that the other two languages mentioned here, it is a highly expressive language and the DataFrame concept is one that has influences many other modern languages (Pandas - Python, DataFrames - Julia etc).</p> <p>The R kernel is fairly simple to download and install via conda package manager, and I would recommend going through this useful article. Unfortunately I discovered that article after the fact, which meant I had to jump over a few hurdles in attempting to get things working on my machine. I will go through the steps I took installing without using conda package manager as it might be useful for people not using conda, but I would recommend if you are, the definitely follow the article I have listed above.</p> <p>Since I already has R installed on my machine, I didn't feel it necessary to re-download r-essntials from conda so I followed the instructions on the IRKernel GitHub readme which instructed me to install the relevant dev tools from within <code>R</code> like so:</p> <p><pre><code>install.packages(c('repr', 'IRdisplay', 'crayon', 'pbdZMQ', 'devtools'))\ndevtools::install_github('IRkernel/IRkernel')\nIRkernel::installspec()  # to register the kernel in the current R installation\n</code></pre> This produced the following error: <pre><code>Error in curl::curl_fetch_memory(url, handle = handle) :\nPeer certificate cannot be authenticated with given CA certificates\n</code></pre> According to stackoverflow this can be easily resolved by setting; <pre><code>library(httr)\nset_config( config( ssl_verifypeer = 0L ) )\n</code></pre> at the R prompt.</p> <p>After installing I received the following error:</p> <p><pre><code>$ which R\n/Users/me/anaconda/bin/R\n\n$ /Users/me/anaconda/bin/R\ndyld: Library not loaded: @rpath/libpcre.1.dylib\n  Referenced from: /Users/me/anaconda/lib/R/lib/libR.dylib\n  Reason: image not found\nTrace/BPT trap: 5\n</code></pre> This is another issue that can be resolved by looking at this SO post.</p> <p>I was able to search for the libpcre.1.dylib file by using the brilliant find command like so:</p> <pre><code>find / -iname \"libpcre.1.dylib\"\n</code></pre> <p>Alternatively one can use fzf fuzzy finder by changing directory to / and running fzf. Once inside the program, simply type: libpcre.1.dylib and it should search for you (this may take a while)</p> <p>Finally, the last step was to make Jupyter see the newly installed kernel by entering the following withing R:</p> <p><pre><code># in R 3.3\nIRkernel::installspec(name = 'ir33', displayname = 'R 3.3')\n# in R 3.2\nIRkernel::installspec(name = 'ir32', displayname = 'R 3.2')\n</code></pre> To ensure this is system-wide, set user to FALSE: <pre><code>IRkernel::installspec(user = FALSE)\n</code></pre> When this is linked you should be off and away and read to use R in Jupyter notebooks.</p> <p></p> <p>R kernel</p>","tags":["python","matlab","notebooks"]},{"location":"blog/2016/11/14/-multilingual-notebooks-a-polyglots-playground/#julia","title":"Julia","text":"<p>Julia is a relatively new programming language designed to be a modern scientific programming language for the 21<sup>st</sup> century.</p> <p>Here is a good guide to get up and running quickly.</p> <p>Julia was definitely the easiest out of the three kernels mentioned here. All that is required is a Julia to be downloaded at version 0.4 or greater on your laptop and to run the following commands from within Julia:</p> <p><pre><code>Pkg.add(\"IJulia\")\n</code></pre> Then: <pre><code>using IJulia\nnotebook()\n</code></pre> will open up a new tab in your default browser.</p> <p>There were a few gotcha I did encounter, the main one being trouble with several plotting packages with Julia.0.5.0 (14-11-2016). I reverted back to Julia 0.4.6 and everything seems to work. I am sure this is only a temporally issue and will be resolved soon.</p> <p>Another hiccup once might have may relate to this issue of the kernel failing to start (Note, this is the same issue mentioned above for the MATLAB kernel)</p> <pre><code>conda remove pyzmq\npip install pyzmq\n</code></pre> <p></p> <p>Julia kernel</p>","tags":["python","matlab","notebooks"]},{"location":"blog/2016/11/14/-multilingual-notebooks-a-polyglots-playground/#final-comments","title":"Final Comments.","text":"<p>I have focused on installing the kernels I felt most useful for Scientific Computing, however a list of other kernels can be found here. Jupyter project is doing wonders in helping many people learn programming and also the research community with sharing code and methodology. I hope this can continue and if you have read this and also think it's awesome, you can find out more on their website. Since Jupyter is an open source project you can always contribute on GitHub, or in other ways through donations!</p>","tags":["python","matlab","notebooks"]},{"location":"blog/2017/11/06/-ah-bufdo-thatll-do/","title":"\u2328\ufe0f Ah <code>:bufdo</code>, that'll do!","text":"<p>A while ago I worked on a method to allow local writing of development logs in markdown, which I could then upload to Github where my supervisor could take a look. Here are the steps..</p> <p></p> <p>\"Real programmers use vim..\"</p> <p>In true vim-fan-boy fashion, I write my development logs in <code>vim</code> and then like to have them render from within <code>vim</code>. To do this I use a plugin called InstantMarkdownPreview. However, the syntax used to render images using this plugin is different from the syntax generally required and used by Github. Therefore, a workaround has been developed such that I write the logs locally on a <code>local</code> branch, and then checkout to master and upon a <code>git push</code> some undercover wizardry works behind the scenes.</p> <p>Considering there could be potentially 100's or markdown files at some point, whatever I do, I should future proof the method for when that day comes when I have many files to edit at once. Enter <code>vim</code>'s <code>:bufdo</code> feature. Below is a brief overview on the workflow required to be able to achieve this.</p> <p>First, one needs a workflow where we have one <code>master</code> branch which will be used to push Github markdown flavoured files to a remote repo, and <code>local</code> which will have InstantMarkdownPreview type of markdown files that can be viewed locally from within <code>vim</code> with the command <code>:InstantMarkdownPreview</code></p> <p>The InstantMarkdown file has the following syntax for displaying images:</p> <pre><code>![](a/raw/b/inc/screenshots/repo-clone-and-sync.png)\n</code></pre> <p>However, Github, does not care for the \"a/raw/b\" that prepends the path of where the image is. So, one can have this is the <code>pre-push</code> file (found in <code>.git/hooks/pre-push</code> that combines the efforts of <code>:bufdo</code> and git hooks to achieve a seamlessly flow from local to Github markdown rendering.</p> <p><code>pre-push</code> looks like:</p> <pre><code>#!/bin/bash\nvim -E -s *.md &lt; cmds.vim &amp;&amp; git commit -am \"Changes made for Github rendering\"\n</code></pre> <p>Where the contents of <code>cmds.vim</code> just contain the commands to be executed in <code>vim</code></p> <pre><code>:bufdo %s/a\\/raw\\/b\\///ge | update\n</code></pre> <p>Now this is all set up, one can do the following, create a local branch which be used write the development logs and to view them in <code>vim</code></p> <pre><code>git co -b local\n</code></pre> <p>Then when the respective files have been written, which paths to images looking like so:</p> <pre><code>![](a/raw/b/inc/screenshots/repo-clone-and-sync.png)\n</code></pre> <p>One cane save these files, <code>git add</code> and <code>git commit</code> them, and then go to master.</p> <pre><code>git co master\ngit merge local -X theirs\n</code></pre> <p>The <code>git merge local -X theirs</code> basically forces the merge even though we will have conflicts relating to the different syntax. A commit message will appear in <code>vim</code> asking to confirm the merge. Saving this, and then doing a <code>git push</code> now does the job! The new syntax that <code>bufdo</code> changes images to looks like the following:</p> <pre><code>![](inc/screenshots/repo-clone-and-sync.png)\n</code></pre> <p>Ready for Github :-)</p> <p>Note: A <code>git push --no-verify</code> may be required if changes are made to the repo that don't invole the workflow described above. <code>--no-verify</code> allows one to skip the git hook file <code>pre-push</code></p>","tags":["unix","vim"]},{"location":"blog/2020/11/06/-awesome-engineering-blogs/","title":"\u2699\ufe0f Awesome Engineering Blogs","text":"<p>The idea is that this list will grow (and possibly shrink) over time as I discover more engineering blogs, and I will use this list as a reference for myself.</p> <p>Much of the list is the same as David's, but I have added a couple, namely Databricks and NVIDIA's engineering blog, and removed some to make it more digestible. There is a very substantial list of Software Engineering blogs by Kilim Choi which I recommend looking into for a more complete list.</p> <ul> <li>Engineering Blog | Facebook Code</li> <li>Engineering | Twitter Blogs</li> <li>Google Developers Blog</li> <li>Google Research Blog</li> <li>Instagram Engineering</li> <li>Blog | LinkedIn Engineering</li> <li>Airbnb Engineering - Nerds</li> <li>Pinterest Engineering</li> <li>GitHub Engineering</li> <li>Code as Craft, Etsy's Engineering Blog</li> <li>Dropbox Tech Blog</li> <li>Engineering at IFTTT</li> <li>eBay Tech Blog</li> <li>Uber Engineering Blog</li> <li>Engineering at Quora</li> <li>Riot Games Engineering</li> <li>Spotify's Engineering and Technology Blog</li> <li>Heroku Engineering Blog</li> <li>Lyft Engineering</li> <li>Cloudera Engineering Blog</li> <li>Stack Overflow Blog</li> <li>PayPal Engineering Blog</li> <li>The Netflix Tech Blog</li> <li>Atlassian Developers</li> <li>KA Engineering (Khan Academy)</li> <li>REA Group Tech Blog</li> <li>Databricks Engineering</li> <li>NVIDIA Developer Blog</li> <li>Confluent Blog</li> </ul> <p>Enjoy!</p>","tags":["unix","vim"]},{"location":"blog/2020/11/06/public-private-repos/","title":"\ud83e\ude9e Mirroring Public Repositories on Github, Privately","text":"<p>This post walks through the steps involved if you want to fork a public Github repository, privately. It will show how to have an open public repository and how to mirror it in a private repository on Github.</p> <p>These steps were inspired from this guide of 'Mirroring a repository' on Github documentation</p> <ol> <li>Create a bare clone of the repository.</li> </ol> <pre><code>14:05:04 \u2714 ~/Github/origin  :: git clone --bare git@github.com:tallamjr/public.git private\nCloning into bare repository 'private'...\nremote: Enumerating objects: 5, done.\nremote: Counting objects: 100% (5/5), done.\nremote: Compressing objects: 100% (4/4), done.\nremote: Total 5 (delta 0), reused 0 (delta 0), pack-reused 0\nReceiving objects: 100% (5/5), 5.47 KiB | 5.47 MiB/s, done.\n</code></pre> <ol> <li>Mirror-push to the new repository.</li> </ol> <pre><code>14:05:45 \u2714 ~/Github/origin  :: cd private/\n14:05:48 \u2714 ~/Github/origin/private (BARE:master) :: git push --mirror git@github.com:tallamjr/private.git\nEnumerating objects: 5, done.\nCounting objects: 100% (5/5), done.\nDelta compression using up to 4 threads\nCompressing objects: 100% (4/4), done.\nWriting objects: 100% (5/5), 5.47 KiB | 5.47 MiB/s, done.\nTotal 5 (delta 0), reused 5 (delta 0)\nTo github.com:tallamjr/private.git\n * [new branch]      master -&gt; master\n</code></pre> <ol> <li>Remove the temporary local repository you created in step 1.</li> </ol> <pre><code>14:06:13 \u2714 ~/Github/origin/private (BARE:master) :: cd ../\n14:06:24 \u2714 ~/Github/origin  :: rm -rf private/\n</code></pre> <ol> <li>Clone newly created 'private' mirrored repository</li> </ol> <pre><code>14:06:30 \u2714 ~/Github/origin  :: git clone git@github.com:tallamjr/private.git\nCloning into 'private'...\nremote: Enumerating objects: 5, done.\nremote: Counting objects: 100% (5/5), done.\nremote: Compressing objects: 100% (4/4), done.\nReceiving objects: 100% (5/5), 5.47 KiB | 5.47 MiB/s, done.\nremote: Total 5 (delta 0), reused 5 (delta 0), pack-reused 0\n14:07:04 \u2714 ~/Github/origin  :: cd private/\n14:07:06 \u2714 ~/Github/origin/private (master) :: git remote -v\norigin  git@github.com:tallamjr/private.git (fetch)\norigin  git@github.com:tallamjr/private.git (push)\n</code></pre> <ol> <li>Allow private repository to 'see' public repository by adding public repository to remote upstream</li> </ol> <pre><code>14:08:04 \u2718 ~/Github/origin/private (master) :: git remote add upstream git@github.com:tallamjr/public.git\n14:09:22 \u2714 ~/Github/origin/private (master) :: git remote -v\norigin  git@github.com:tallamjr/private.git (fetch)\norigin  git@github.com:tallamjr/private.git (push)\nupstream        git@github.com:tallamjr/public.git (fetch)\nupstream        git@github.com:tallamjr/public.git (push)\n</code></pre> <p>NOTE Completing steps 1..5 should give you everything you need. Steps 6..10 go further with some sanity checks to ensure everything has indeed worked, having said that, step 9 is recommended.</p> <ol> <li>Clone public repository alongside private to test set-up by editing the public README.md (OPTIONAL)</li> </ol> <pre><code>14:10:25 \u2714 ~/Github/origin/private (master) :: cd ../\n14:10:41 \u2714 ~/Github/origin  :: git clone git@github.com:tallamjr/public.git\nCloning into 'public'...\nremote: Enumerating objects: 5, done.\nremote: Counting objects: 100% (5/5), done.\nremote: Compressing objects: 100% (4/4), done.\nremote: Total 5 (delta 0), reused 0 (delta 0), pack-reused 0\nReceiving objects: 100% (5/5), 5.47 KiB | 5.47 MiB/s, done.\n14:10:50 \u2714 ~/Github/origin  :: cd public/\n14:10:52 \u2714 ~/Github/origin/public (master) :: vim README.md\n14:18:15 \u2714 ~/Github/origin/public (master) :: git status\nOn branch master\nYour branch is up to date with 'origin/master'.\n\nChanges not staged for commit:\n  (use \"git add &lt;file&gt;...\" to update what will be committed)\n  (use \"git restore &lt;file&gt;...\" to discard changes in working directory)\n        modified:   README.md\n\nno changes added to commit (use \"git add\" and/or \"git commit -a\")\n</code></pre> <ol> <li>Add, commit and push recent changes to the public README.md (OPTIONAL)</li> </ol> <pre><code>14:19:13 \u2714 ~/Github/origin/public (master) :: git add README.md\n14:19:42 \u2714 ~/Github/origin/public (master) :: git commit\n[master 881d666] Updating README with instructions\n 1 file changed, 82 insertions(+), 1 deletion(-)\n rewrite README.md (100%)\n14:20:36 \u2714 ~/Github/origin/public (master) :: git status\nOn branch master\nYour branch is ahead of 'origin/master' by 1 commit.\n  (use \"git push\" to publish your local commits)\n\nnothing to commit, working tree clean\n14:20:37 \u2714 ~/Github/origin/public (master) :: git push\nEnumerating objects: 5, done.\nCounting objects: 100% (5/5), done.\nDelta compression using up to 4 threads\nCompressing objects: 100% (3/3), done.\nWriting objects: 100% (3/3), 1.04 KiB | 1.04 MiB/s, done.\nTotal 3 (delta 1), reused 0 (delta 0)\nremote: Resolving deltas: 100% (1/1), completed with 1 local object.\nTo github.com:tallamjr/public.git\n   ba0bb9d..881d666  master -&gt; master\n</code></pre> <ol> <li>Go into the private repository and check 'status' (OPTIONAL). Notice with <code>git log</code> we only see a    single commit from when the private repository was instantiated.</li> </ol> <pre><code>14:22:41 \u2714 ~/Github/origin/public (master) :: cd ../private/\n14:22:45 \u2714 ~/Github/origin/private (master) :: git status\nOn branch master\nYour branch is up to date with 'origin/master'.\n\nnothing to commit, working tree clean\n14:22:51 \u2714 ~/Github/origin/private (master) :: git log\ncommit ba0bb9d112de731e246741436f7b4160419b9c4a (HEAD -&gt; master, origin/master, origin/HEAD)\nAuthor: Tarek &lt;tallamjr@users.noreply.github.com&gt;\nDate:   Tue Feb 25 14:03:20 2020 +0000\n\n    Initial commit\n14:22:54 \u2714 ~/Github/origin/private (master) ::\n</code></pre> <ol> <li>Bring in changes made from public repository into private repository i.e. sync private with the    public repository (RECOMMENDED)</li> </ol> <p>NOTE <code>git update</code> is an alias for:</p> <pre><code>git pull --rebase upstream master --ff-only &amp;&amp; git fetch --all\n</code></pre> <pre><code>14:24:00 \u2714 ~/Github/origin/private (master) :: git update\nremote: Enumerating objects: 8, done.\nremote: Counting objects: 100% (8/8), done.\nremote: Compressing objects: 100% (4/4), done.\nremote: Total 6 (delta 3), reused 5 (delta 2), pack-reused 0\nUnpacking objects: 100% (6/6), 1.87 KiB | 319.00 KiB/s, done.\nFrom github.com:tallamjr/public\n * branch            master     -&gt; FETCH_HEAD\n * [new branch]      master     -&gt; upstream/master\nUpdating ba0bb9d..372ebcf\nFast-forward\n README.md | 118 ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++-\n 1 file changed, 117 insertions(+), 1 deletion(-)\nCurrent branch master is up to date.\nFetching origin\nFetching upstream\n14:24:08 \u2714 ~/Github/origin/private (master) :: git log\ncommit 372ebcf4d7aa0377d085bd47c0ccf478201851ef (HEAD -&gt; master, upstream/master)\nAuthor: Tarek Allam &lt;t.allam.jr@gmail.com&gt;\nDate:   Tue Feb 25 14:21:54 2020 +0000\n\n    [FIXUP]\n\n    Adding further instructions\n            modified:   README.md\n\ncommit 881d666d86b912e4e687069bf422f4b31dcd9965\nAuthor: Tarek Allam &lt;t.allam.jr@gmail.com&gt;\nDate:   Tue Feb 25 14:19:44 2020 +0000\n\n    Updating README with instructions\n\n    How to mirror a public repo, privately\n\n            modified:   README.md\n\ncommit ba0bb9d112de731e246741436f7b4160419b9c4a (origin/master, origin/HEAD)\nAuthor: Tarek &lt;tallamjr@users.noreply.github.com&gt;\nDate:   Tue Feb 25 14:03:20 2020 +0000\n\n    Initial commit\n14:24:12 \u2714 ~/Github/origin/private (master) :: git push\nEnumerating objects: 8, done.\nCounting objects: 100% (8/8), done.\nDelta compression using up to 4 threads\nCompressing objects: 100% (6/6), done.\nWriting objects: 100% (6/6), 1.72 KiB | 440.00 KiB/s, done.\nTotal 6 (delta 3), reused 0 (delta 0)\nremote: Resolving deltas: 100% (3/3), completed with 1 local object.\nTo github.com:tallamjr/private.git\n   ba0bb9d..372ebcf  master -&gt; master\n</code></pre> <p>Above we can see that we now have the 'extra' commits that were made in the public repository now in the private repository too.</p> <ol> <li>This point is more a discussion about issues and branching models (OPTIONAL)</li> </ol> <p>It is best practise to relate all branches to a specific issue, i.e. <code>issue/3/some-issue</code> and also with features, <code>features/45/some-awesome-feature</code> which can further evolves into <code>feature/45/issue/51/an-issue-with-awesome-feature</code></p> <p>When one has the set up described above, you would mostly likely have issues that are specific for the public and potentially issues that are specific for the private (usually it would be the case when forking, the created issue branch will always relate to the upstream version as this is the 'main' repository. However, in the case with public and private, there may be a case where issues in the private repository are indeed 'private' and would be best kept 'in-house'. For this situation, one can have a system like so:</p> <p>public \u2192</p> <pre><code>- issue/&lt;number&gt;/short-token-description\n- feature/&lt;number&gt;/short-token-description\n- feature/&lt;number&gt;/issue/&lt;number&gt;/short-token-description\n</code></pre> <p>private \u2192</p> <pre><code>- pv/issue/&lt;number&gt;/short-token-description\n- pv/feature/&lt;number&gt;/short-token-description\n- pv/feature/&lt;number&gt;/issue/&lt;number&gt;/short-token-description\n</code></pre> <ul> <li>Finally we can rebase a commit on the public repository and ensure this is reflected in the   private repository. Again, this is just a system sanity check.</li> </ul> <pre><code>14:50:51 \u2714 ~/Github/origin/public (master) :: git rebase -i HEAD~3\n</code></pre> <pre><code>pick 881d666 Updating README with instructions\nf 372ebcf [FIXUP]\npick 848079d Update with final set of instructions\n\n# Rebase ba0bb9d..848079d onto ba0bb9d (3 commands)\n#\n# Commands:\n# p, pick &lt;commit&gt; = use commit\n# r, reword &lt;commit&gt; = use commit, but edit the commit message\n# e, edit &lt;commit&gt; = use commit, but stop for amending\n# s, squash &lt;commit&gt; = use commit, but meld into previous commit\n# f, fixup &lt;commit&gt; = like \"squash\", but discard this commit's log message\n# x, exec &lt;command&gt; = run command (the rest of the line) using shell\n# b, break = stop here (continue rebase later with 'git rebase --continue')\n# d, drop &lt;commit&gt; = remove commit\n# l, label &lt;label&gt; = label current HEAD with a name\n# t, reset &lt;label&gt; = reset HEAD to a label\n# m, merge [-C &lt;commit&gt; | -c &lt;commit&gt;] &lt;label&gt; [# &lt;oneline&gt;]\n# .       create a merge commit using the original merge commit's\n# .       message (or the oneline, if no original merge commit was\n# .       specified). Use -c &lt;commit&gt; to reword the commit message.\n#\n# These lines can be re-ordered; they are executed from top to bottom.\n#\n# If you remove a line here THAT COMMIT WILL BE LOST.\n#\n# However, if you remove everything, the rebase will be aborted.\n#\n# Note that empty commits are commented out\n</code></pre> <pre><code>Successfully rebased and updated refs/heads/master.\n14:50:59 \u2714 ~/Github/origin/public (master) :: git log --oneline\nd1293c2 (HEAD -&gt; master) Update with final set of instructions\nb727c18 Updating README with instructions\nba0bb9d Initial commit\n14:51:03 \u2714 ~/Github/origin/public (master) :: git push -f\nEnumerating objects: 8, done.\nCounting objects: 100% (8/8), done.\nDelta compression using up to 4 threads\nCompressing objects: 100% (6/6), done.\nWriting objects: 100% (6/6), 3.33 KiB | 3.33 MiB/s, done.\nTotal 6 (delta 3), reused 0 (delta 0)\nremote: Resolving deltas: 100% (3/3), completed with 1 local object.\nTo github.com:tallamjr/public.git\n + 848079d...d1293c2 master -&gt; master (forced update)\n14:51:12 \u2714 ~/Github/origin/public (master) :: cd ../private/\n14:51:15 \u2714 ~/Github/origin/private (master) :: git update\nremote: Enumerating objects: 8, done.\nremote: Counting objects: 100% (8/8), done.\nremote: Compressing objects: 100% (3/3), done.\nremote: Total 6 (delta 3), reused 6 (delta 3), pack-reused 0\nUnpacking objects: 100% (6/6), 3.31 KiB | 483.00 KiB/s, done.\nFrom github.com:tallamjr/public\n * branch            master     -&gt; FETCH_HEAD\n + 848079d...d1293c2 master     -&gt; upstream/master  (forced update)\nFirst, rewinding head to replay your work on top of it...\nFetching origin\nFetching upstream\n14:51:21 \u2714 ~/Github/origin/private (master) :: git log --oneline\nd1293c2 (HEAD -&gt; master, upstream/master) Update with final set of instructions\nb727c18 Updating README with instructions\nba0bb9d Initial commit\n14:51:23 \u2714 ~/Github/origin/private (master) :: git push -f\nEnumerating objects: 8, done.\nCounting objects: 100% (8/8), done.\nDelta compression using up to 4 threads\nCompressing objects: 100% (6/6), done.\nWriting objects: 100% (6/6), 3.33 KiB | 1.11 MiB/s, done.\nTotal 6 (delta 3), reused 0 (delta 0)\nremote: Resolving deltas: 100% (3/3), completed with 1 local object.\nTo github.com:tallamjr/private.git\n + 848079d...d1293c2 master -&gt; master (forced update)\n14:51:30 \u2714 ~/Github/origin/private (master) ::\n</code></pre>","tags":["unix","vim"]},{"location":"blog/2020/11/06/-pyspark-by-example/","title":"\u26a1\ufe0f PySpark by Example","text":"<p>Here are my worked examples from the very useful LinkedIn Learning course: PySpark by Example by Jonathan Fernandes : https://www.linkedin.com/learning/apache-pyspark-by-example</p> <p> </p>","tags":["unix","vim"]},{"location":"blog/2020/11/06/-pyspark-by-example/#learning-pyspark-by-example","title":"Learning PySpark by Example","text":"<p>Over the past 12 months or so I have been learning and playing with Apache Spark. I went through the brilliant book by Bill Chambers and Matei Zaharia, Spark: The Definitive Guide, that covers Spark in depth and gives plenty of code snippets one can try out in the <code>spark-shell</code>. Whilst the book is indeed very detailed and provides great examples, the datasets that are included for you to get your hands on are on the order of <code>Mb</code>'s (with the exception of the <code>activity-data</code> dataset used for the Streaming examples).</p> <pre><code>$ du -sh data/* | sort -rh\n1.2G    data/activity-data\n90M     data/retail-data\n71M     data/deep-learning-images\n42M     data/bike-data\n208K    data/flight-data\n104K    data/sample_libsvm_data.txt\n32K     data/sample_movielens_ratings.txt\n32K     data/regression\n32K     data/multiclass-classification\n32K     data/clustering\n32K     data/binary-classification\n12K     data/simple-ml-integers\n12K     data/flight-data-hive\n8.0K    data/simple-ml\n4.0K    data/simple-ml-scaling\n4.0K    data/README.md\n</code></pre> <p>For this reason, I wanted to try out PySpark by Example that plays with the City of Chicago's <code>reported-crimes.csv</code> dataset which is around <code>1.6Gb</code>.</p> <p>Another reason for why that course and the related dataset was appealing, was I could use it as an excuse to explore the plotting capabilities of Ploty, an interactive library for plotting data. This dataset had location data combined with distributions of data.</p>","tags":["unix","vim"]},{"location":"blog/2020/11/06/-pyspark-by-example/#the-data","title":"The Data","text":"<p>As mentioned, in the course, the City of Chicago, reported crimes data was used.</p> <pre><code>$ wget https://data.cityofchicago.org/api/views/ijzp-q8t2/rows.csv?accessType=DOWNLOAD\n$ mv rows.csv\\?accessType\\=DOWNLOAD data/reported-crimes.csv\n</code></pre> <pre><code>&gt;&gt;&gt; from pyspark.sql.functions import to_timestamp,col,lit\n&gt;&gt;&gt; rc = spark.read.csv('data/reported-crimes.csv',header=True).withColumn('Date',to_timestamp(col('Date'),'MM/dd/yyyy hh:mm:ss a')).filter(col('Date') &lt;= lit('2018-11-11'))\n&gt;&gt;&gt; rc.show(5)\n</code></pre> <pre><code>+--------+-----------+-------------------+--------------------+----+-------------------+--------------------+--------------------+------+--------+----+--------+----+--------------+--------+------------+------------+----+--------------------+--------+---------+--------+\n|      ID|Case Number|               Date|               Block|IUCR|       Primary Type|         Description|Location Description|Arrest|Domestic|Beat|District|Ward|Community Area|FBI Code|X Coordinate|Y Coordinate|Year|          Updated On|Latitude|Longitude|Location|\n+--------+-----------+-------------------+--------------------+----+-------------------+--------------------+--------------------+------+--------+----+--------+----+--------------+--------+------------+------------+----+--------------------+--------+---------+--------+\n|11034701|   JA366925|2001-01-01 11:00:00|     016XX E 86TH PL|1153| DECEPTIVE PRACTICE|FINANCIAL IDENTIT...|           RESIDENCE| false|   false|0412|     004|   8|            45|      11|        null|        null|2001|08/05/2017 03:50:...|    null|     null|    null|\n|11227287|   JB147188|2017-10-08 03:00:00|  092XX S RACINE AVE|0281|CRIM SEXUAL ASSAULT|      NON-AGGRAVATED|           RESIDENCE| false|   false|2222|     022|  21|            73|      02|        null|        null|2017|02/11/2018 03:57:...|    null|     null|    null|\n|11227583|   JB147595|2017-03-28 14:00:00|     026XX W 79TH ST|0620|           BURGLARY|      UNLAWFUL ENTRY|               OTHER| false|   false|0835|     008|  18|            70|      05|        null|        null|2017|02/11/2018 03:57:...|    null|     null|    null|\n|11227293|   JB147230|2017-09-09 20:17:00|060XX S EBERHART AVE|0810|              THEFT|           OVER $500|           RESIDENCE| false|   false|0313|     003|  20|            42|      06|        null|        null|2017|02/11/2018 03:57:...|    null|     null|    null|\n|11227634|   JB147599|2017-08-26 10:00:00| 001XX W RANDOLPH ST|0281|CRIM SEXUAL ASSAULT|      NON-AGGRAVATED|         HOTEL/MOTEL| false|   false|0122|     001|  42|            32|      02|        null|        null|2017|02/11/2018 03:57:...|    null|     null|    null|\n+--------+-----------+-------------------+--------------------+----+-------------------+--------------------+--------------------+------+--------+----+--------+----+--------------+--------+------------+------------+----+--------------------+--------+---------+--------+\nonly showing top 5 rows\n</code></pre> <pre><code>&gt;&gt;&gt; rc.columns\n['ID', 'Case Number', 'Date', 'Block', 'IUCR', 'Primary Type', 'Description', 'Location Description', 'Arrest', 'Domestic', 'Beat', 'District', 'Ward', 'Community Area', 'FBI Code', 'X Coordinate', 'Y Coordinate', 'Year', 'Updated On', 'Latitude', 'Longitude', 'Location']\n</code></pre> <p>Another dataset is used,</p> <pre><code>$ wget -O data/police-stations.csv https://data.cityofchicago.org/api/views/z8bn-74gv/rows.csv?accessType=DOWNLOAD\n</code></pre> <pre><code>&gt;&gt;&gt; ps = spark.read.csv(\"data/police-stations.csv\", header=True)\n&gt;&gt;&gt; ps.show(5, truncate=False)\n</code></pre> <pre><code>+------------+--------------+--------------------+-------+-----+-----+-------------------------------------------------------------------------------+------------+------------+------------+------------+------------+-----------+------------+-------------------------------+\n|DISTRICT    |DISTRICT NAME |ADDRESS             |CITY   |STATE|ZIP  |WEBSITE                                                                        |PHONE       |FAX         |TTY         |X COORDINATE|Y COORDINATE|LATITUDE   |LONGITUDE   |LOCATION                       |\n+------------+--------------+--------------------+-------+-----+-----+-------------------------------------------------------------------------------+------------+------------+------------+------------+------------+-----------+------------+-------------------------------+\n|Headquarters|Headquarters  |3510 S Michigan Ave |Chicago|IL   |60653|http://home.chicagopolice.org                                                  |null        |null        |null        |1177731.401 |1881697.404 |41.83070169|-87.62339535|(41.8307016873, -87.6233953459)|\n|1           |Central       |1718 S State St     |Chicago|IL   |60616|http://home.chicagopolice.org/community/districts/1st-district-central/        |312-745-4290|312-745-3694|312-745-3693|1176569.052 |1891771.704 |41.85837259|-87.62735617|(41.8583725929, -87.627356171) |\n|6           |Gresham       |7808 S Halsted St   |Chicago|IL   |60620|http://home.chicagopolice.org/community/districts/6th-district-gresham/        |312-745-3617|312-745-3649|312-745-3639|1172283.013 |1853022.646 |41.75213684|-87.64422891|(41.7521368378, -87.6442289066)|\n|11          |Harrison      |3151 W Harrison St  |Chicago|IL   |60612|http://home.chicagopolice.org/community/districts/11th-district-harrison/      |312-746-8386|312-746-4281|312-746-5151|1155244.069 |1897148.755 |41.87358229|-87.70548813|(41.8735822883, -87.705488126) |\n|16          |Jefferson Park|5151 N Milwaukee Ave|Chicago|IL   |60630|http://home.chicagopolice.org/community/districts/16th-district-jefferson-park/|312-742-4480|312-742-4421|312-742-4423|1138480.758 |1933660.473 |41.97409445|-87.76614884|(41.9740944511, -87.7661488432)|\n+------------+--------------+--------------------+-------+-----+-----+-------------------------------------------------------------------------------+------------+------------+------------+------------+------------+-----------+------------+-------------------------------+\nonly showing top 5 rows\n</code></pre> <pre><code>&gt;&gt;&gt; ps.columns\n['DISTRICT', 'DISTRICT NAME', 'ADDRESS', 'CITY', 'STATE', 'ZIP', 'WEBSITE', 'PHONE', 'FAX', 'TTY', 'X COORDINATE', 'Y COORDINATE', 'LATITUDE', 'LONGITUDE', 'LOCATION']\n</code></pre>","tags":["unix","vim"]},{"location":"blog/2020/11/06/-pyspark-by-example/#exploratory-data-analysis-and-challenge-questions","title":"Exploratory Data Analysis and Challenge Questions","text":"<p>Before we do this, let's cache the dataset in memory for faster querying, this will alleviate us from the burden of reading from disk each time we want to run a query.</p> <pre><code>&gt;&gt;&gt; rc.cache()\nDataFrame[ID: string, Case Number: string, Date: timestamp, Block: string, IUCR: string, Primary Type: string, Description: string, Location Description: string, Arrest: string, Domestic: string, Beat: string, District: string, Ward: string, Community Area: string, FBI Code: string, X Coordinate: string, Y Coordinate: string, Year: string, Updated On: string, Latitude: string, Longitude: string, Location: string]\n&gt;&gt;&gt; rc.count()\n6752020\n</code></pre> <p>Note, the <code>cache()</code> command is evaluated lazily, so an action is required to execute it. Here we simply do a <code>count()</code> action to ensure <code>cache()</code> is run.</p>","tags":["unix","vim"]},{"location":"blog/2020/11/06/-pyspark-by-example/#display-only-the-first-4-rows-of-the-column-names-case-number-date-and-arrest","title":"Display only the first 4 rows of the column names Case Number, Date and Arrest","text":"<pre><code>&gt;&gt;&gt; rc.select('Case Number', 'Date', 'Arrest').show(4)\n+-----------+-------------------+------+\n|Case Number|               Date|Arrest|\n+-----------+-------------------+------+\n|   JA366925|2001-01-01 11:00:00| false|\n|   JB147188|2017-10-08 03:00:00| false|\n|   JB147595|2017-03-28 14:00:00| false|\n|   JB147230|2017-09-09 20:17:00| false|\n+-----------+-------------------+------+\nonly showing top 4 rows\n</code></pre>","tags":["unix","vim"]},{"location":"blog/2020/11/06/-pyspark-by-example/#what-are-the-top-10-number-of-reported-crimes-by-primary-type-in-descending-order-of-occurrence","title":"What are the top 10 number of reported crimes by Primary type, in descending order of occurrence?","text":"<pre><code>&gt;&gt;&gt; rc.groupBy('Primary Type').count().show(5)\n+--------------------+-----+\n|        Primary Type|count|\n+--------------------+-----+\n|OFFENSE INVOLVING...|45709|\n|CRIMINAL SEXUAL A...|  333|\n|            STALKING| 3384|\n|PUBLIC PEACE VIOL...|47780|\n|           OBSCENITY|  582|\n+--------------------+-----+\nonly showing top 5 rows\n</code></pre> <p>But we want these in order, so:</p> <pre><code>&gt;&gt;&gt; rc.groupBy('Primary Type').count().orderBy('count', ascending=False).show(10)\n+-------------------+-------+\n|       Primary Type|  count|\n+-------------------+-------+\n|              THEFT|1418293|\n|            BATTERY|1232001|\n|    CRIMINAL DAMAGE| 771399|\n|          NARCOTICS| 711609|\n|      OTHER OFFENSE| 418802|\n|            ASSAULT| 418479|\n|           BURGLARY| 388009|\n|MOTOR VEHICLE THEFT| 314101|\n| DECEPTIVE PRACTICE| 265567|\n|            ROBBERY| 255566|\n+-------------------+-------+\nonly showing top 10 rows\n</code></pre>","tags":["unix","vim"]},{"location":"blog/2020/11/06/-pyspark-by-example/#what-percentage-of-reported-crimes-resulted-in-an-arrest","title":"What percentage of reported crimes resulted in an arrest?","text":"<pre><code>&gt;&gt;&gt; rc.select('Arrest').distinct().show()\n+------+\n|Arrest|\n+------+\n| false|\n|  true|\n+------+\n</code></pre> <pre><code>&gt;&gt;&gt; rc.where(col('Arrest') == 'true').count() / rc.select('Arrest').count()\n0.2775467193521346\n</code></pre>","tags":["unix","vim"]},{"location":"blog/2020/11/06/-pyspark-by-example/#what-are-the-top-3-locations-for-reported-crimes","title":"What are the top 3 locations for reported crimes?","text":"<pre><code>&gt;&gt;&gt; rc.groupBy('Location Description').count().orderBy('count', ascending=False).show(3)\n+--------------------+-------+\n|Location Description|  count|\n+--------------------+-------+\n|              STREET|1770359|\n|           RESIDENCE|1144528|\n|           APARTMENT| 698091|\n+--------------------+-------+\nonly showing top 3 rows\n</code></pre>","tags":["unix","vim"]},{"location":"blog/2020/11/06/-pyspark-by-example/#what-is-the-most-frequently-reported-non-criminal-activity","title":"What is the most frequently reported non-criminal activity?","text":"<pre><code>&gt;&gt;&gt; rc.select('Primary Type').distinct().count()\n36\n</code></pre> <p>36 types of crime..</p> <pre><code>&gt;&gt;&gt; rc.select('Primary Type').distinct().orderBy(col('Primary Type')).show(36, truncate=False)\n+---------------------------------+\n|Primary Type                     |\n+---------------------------------+\n|ARSON                            |\n|ASSAULT                          |\n|BATTERY                          |\n|BURGLARY                         |\n|CONCEALED CARRY LICENSE VIOLATION|\n|CRIM SEXUAL ASSAULT              |\n|CRIMINAL DAMAGE                  |\n|CRIMINAL SEXUAL ASSAULT          |\n|CRIMINAL TRESPASS                |\n|DECEPTIVE PRACTICE               |\n|DOMESTIC VIOLENCE                |\n|GAMBLING                         |\n|HOMICIDE                         |\n|HUMAN TRAFFICKING                |\n|INTERFERENCE WITH PUBLIC OFFICER |\n|INTIMIDATION                     |\n|KIDNAPPING                       |\n|LIQUOR LAW VIOLATION             |\n|MOTOR VEHICLE THEFT              |\n|NARCOTICS                        |\n|NON - CRIMINAL                   |\n|NON-CRIMINAL                     |\n|NON-CRIMINAL (SUBJECT SPECIFIED) |\n|OBSCENITY                        |\n|OFFENSE INVOLVING CHILDREN       |\n|OTHER NARCOTIC VIOLATION         |\n|OTHER OFFENSE                    |\n|PROSTITUTION                     |\n|PUBLIC INDECENCY                 |\n|PUBLIC PEACE VIOLATION           |\n|RITUALISM                        |\n|ROBBERY                          |\n|SEX OFFENSE                      |\n|STALKING                         |\n|THEFT                            |\n|WEAPONS VIOLATION                |\n+---------------------------------+\n</code></pre> <pre><code>&gt;&gt;&gt; nc = rc.filter( (col('Primary Type') == 'NON - CRIMINAL') | (col('Primary Type') == 'NON-CRIMINAL ') | (col('Primary Type') == 'NON-CRIMINAL (SUBJECT SPECIFIED)') )\n&gt;&gt;&gt; nc.show(5, truncate=False)\n+--------+-----------+-------------------+---------------------+----+--------------+-----------------+-------------------------------+------+--------+----+--------+----+--------------+--------+------------+------------+----+----------------------+------------+-------------+-----------------------------+\n|ID      |Case Number|Date               |Block                |IUCR|Primary Type  |Description      |Location Description           |Arrest|Domestic|Beat|District|Ward|Community Area|FBI Code|X Coordinate|Y Coordinate|Year|Updated On            |Latitude    |Longitude    |Location                     |\n+--------+-----------+-------------------+---------------------+----+--------------+-----------------+-------------------------------+------+--------+----+--------+----+--------------+--------+------------+------------+----+----------------------+------------+-------------+-----------------------------+\n|10062441|HY250685   |2015-05-07 13:20:00|012XX S HARDING AVE  |5114|NON - CRIMINAL|FOID - REVOCATION|RESIDENCE                      |false |false   |1011|010     |24  |29            |26      |1150243     |1894129     |2015|02/10/2018 03:50:01 PM|41.865394646|-87.723928428|(41.865394646, -87.723928428)|\n|10064717|HY253344   |2015-05-08 13:15:00|051XX S WENTWORTH AVE|5114|NON - CRIMINAL|FOID - REVOCATION|POLICE FACILITY/VEH PARKING LOT|false |false   |0225|002     |3   |37            |26      |1175826     |1871120     |2015|02/10/2018 03:50:01 PM|41.80171934 |-87.630703621|(41.80171934, -87.630703621) |\n|10072565|HY261001   |2015-05-14 10:30:00|006XX N WELLS ST     |5114|NON - CRIMINAL|FOID - REVOCATION|STREET                         |false |false   |1832|018     |42  |8             |26      |1174623     |1904537     |2015|02/10/2018 03:50:01 PM|41.89344506 |-87.634117632|(41.89344506, -87.634117632) |\n|10109156|HY297801   |2015-06-12 09:00:00|053XX S NEVA AVE     |5114|NON - CRIMINAL|FOID - REVOCATION|RESIDENCE                      |false |false   |0811|008     |23  |56            |26      |1129584     |1868411     |2015|02/10/2018 03:50:01 PM|41.795197456|-87.800355525|(41.795197456, -87.800355525)|\n|10115077|HY304017   |2015-06-16 19:00:00|081XX S WHIPPLE ST   |5114|NON - CRIMINAL|FOID - REVOCATION|RESIDENCE                      |false |false   |0835|008     |18  |70            |26      |1157460     |1850515     |2015|02/10/2018 03:50:01 PM|41.745568408|-87.698616805|(41.745568408, -87.698616805)|\n+--------+-----------+-------------------+---------------------+----+--------------+-----------------+-------------------------------+------+--------+----+--------+----+--------------+--------+------------+------------+----+----------------------+------------+-------------+-----------------------------+\nonly showing top 5 rows\n&gt;&gt;&gt; nc.groupBy(col('Description')).count().orderBy('count', ascending=False).show(truncate=False)\n+--------------------------------------+-----+\n|Description                           |count|\n+--------------------------------------+-----+\n|FOID - REVOCATION                     |38   |\n|NOTIFICATION OF CIVIL NO CONTACT ORDER|9    |\n+--------------------------------------+-----+\n</code></pre>","tags":["unix","vim"]},{"location":"blog/2020/11/06/-pyspark-by-example/#using-a-bar-chart-plot-which-day-of-the-week-has-the-most-number-of-reported-crime","title":"Using a bar chart, plot which day of the week has the most number of reported crime.","text":"<pre><code>&gt;&gt;&gt; from pyspark.sql.functions import count, avg\n&gt;&gt;&gt; ss = rc.groupBy(dayofweek(col('Date')), date_format(col('Date'), 'E')).agg(count(\"*\")).show()\n+---------------+--------------------+--------+\n|dayofweek(Date)|date_format(Date, E)|count(1)|\n+---------------+--------------------+--------+\n|              2|                 Mon|  952646|\n|              6|                 Fri| 1016882|\n|              1|                 Sun|  911174|\n|              5|                 Thu|  964457|\n|              4|                 Wed|  973801|\n|              3|                 Tue|  967965|\n|              7|                 Sat|  965095|\n+---------------+--------------------+--------+\n\n&gt;&gt;&gt; ss = rc.groupBy(dayofweek(col('Date')), date_format(col('Date'), 'E')).count().orderBy('dayofweek(Date)')\n&gt;&gt;&gt; ss.show()\n+---------------+--------------------+-------+\n|dayofweek(Date)|date_format(Date, E)|  count|\n+---------------+--------------------+-------+\n|              1|                 Sun| 911174|\n|              2|                 Mon| 952646|\n|              3|                 Tue| 967965|\n|              4|                 Wed| 973801|\n|              5|                 Thu| 964457|\n|              6|                 Fri|1016882|\n|              7|                 Sat| 965095|\n+---------------+--------------------+-------+\n\n&gt;&gt;&gt; type(ss)\n&lt;class 'pyspark.sql.dataframe.DataFrame'&gt;\n&gt;&gt;&gt; sbar = ss.withColumn(\"Day of Week\", col(\"date_format(Date, E)\"))\n</code></pre> <pre><code>&gt;&gt;&gt; import plotly.express as px\n&gt;&gt;&gt; fig = px.bar(sbar.toPandas(), x=\"Day of Week\", y=\"count\")\n&gt;&gt;&gt; fig.show()\n</code></pre>","tags":["unix","vim"]},{"location":"blog/2020/11/06/-pyspark-by-example/#maps","title":"Maps","text":"<p>Now let's explore <code>plotly</code>'s functionality for plotting data with lat, long co-ordinates...</p> <p>Below is a figure made from a sub-sample of the data of 100 rows.</p> <p>Let's scale this up! Now we'll try for 50, 000 points:</p> <p>Awesome! Now I would have loved to show all points (800,000) but my laptop crashes each time I try, I assume this is simply a memory issue and on other laptops might well be fine..</p>","tags":["unix","vim"]},{"location":"blog/2020/11/06/-pyspark-by-example/#conclusions","title":"Conclusions","text":"<p>Playing with this dataset has been fun and it has been interesting to follow the course at the same time exploring <code>plotly</code>. I've put a few points on things I learned whilst writing this post. Overall, I have learned at lot and look forward to exploring more \"big-data\"-sets with <code>pyspark</code> and <code>plotly</code></p>","tags":["unix","vim"]},{"location":"blog/2020/11/06/-pyspark-by-example/#things-i-learned-along-the-way","title":"Things I Learned Along the Way","text":"<ol> <li> <p>A few key things-I-learned during this post was how to embed interactive <code>plotly</code> figures into     markdown such that they can be rendered into the blog with ease.</p> <pre><code>This can simply be done using the `to_html(..)` function:\n```python\nimport plotly\nplotly.io.to_html(fig, include_plotlyjs=False, full_html=False)\n```\nThis spits out a `&lt;div&gt;` element one can then place into their desired markdown, which will then translate as\nrendered `HTML`.\n\nTo ensure just the `&lt;div&gt;` element is returned, `full_html=False` is required. Another thing to\nremember is that this will return the element as a string, so the leading and trailing apostophe's\nthat make it a string need to be removed. In the process of discovering this, a potential \"bug\" was\nfound in this actual function, resulting in excessive `\\n` characters being generated. So, the\nactual function that has been used for this post is:\n\n```python\nimport importlib.util\nspec = importlib.util.spec_from_file_location(\"plotly\", \"/Users/tallamjr/github/forks/plotly.py/packages/python/plotly/plotly/io/_html.py\")\nfoo = importlib.util.module_from_spec(spec)\nspec.loader.exec_module(foo)\nfoo.to_html(fig, include_plotlyjs=False, full_html=False)\n```\n\nWhich points to a forked version of the `plotly` codebase while I have an outstanding\n[PR](https://github.com/plotly/plotly.py/pull/2469) waiting to be reviewed.\n\nA final thing to mention, is that in order for all of the plots above to show up at all, even with\nthe `&lt;div&gt;` elements, one needs to make sure to include the necessary Javascript tags. Therefore, in\nthe `head.html` file for this blog, there exists:\n\n```less\n$ sed -n 14,17p layouts/partials/head.html\n&lt;!-- Plotly embeddings\nREF: http://www.kellieottoboni.com/posts/2017/08/plotly-markup/\n================================================== --&gt;\n&lt;script src=\"https://cdn.plot.ly/plotly-latest.min.js\"&gt;&lt;/script&gt;\n```\n</code></pre> </li> <li> <p>Another thing I discovered was how to allow for better formatting of the <code>.show()</code> command on Spark DataFrames. My approach is explained in this StackOverflow post for pyspark show dataframe as table with horizontal scroll in ipython notebook and Improve PySpark DataFrame.show output to fit Jupyter notebook:</p> <pre><code>Adding to the answers given above by @karan-singla and @vijay-jangir, a handy one-liner to comment\nout the `white-space: pre-wrap` styling can be done like so:\n\n$ awk -i inplace '/pre-wrap/ {$0=\"/*\"$0\"*/\"}1' $(dirname `python -c \"import notebook as nb;print(nb.__file__)\"`)/static/style/style.min.css\n\nThis translates as; use `awk` to update _inplace_ lines that contain `pre-wrap` to be surrounded by\n`*/ -- */` i.e. comment out, on the file found in `styles.css` found in your working Python\nenvironment.\n\nThis, in theory, can then be used as an alias if one uses multiple environments, say with Anaconda.\n- https://stackoverflow.com/a/24884616/4521950\n- https://stackoverflow.com/questions/16529716/save-modifications-in-place-with-awk\n</code></pre> </li> <li> <p>Finally, this is not necessarily something I learned during this post, but it opened my eyes to     the possibilities that are available when using <code>nbconvert</code> The notebook for this post has been     rendered here using the following command:</p> <pre><code> $ jupyter nbconvert --ExecutePreprocessor.kernel_name=python --ExecutePreprocessor.timeout=600 --to html --execute PySpark-by-Example.ipynb --output-dir /Users/tallamjr/www/blog/static/notebooks\n</code></pre> <p>After look for ways to link point number 1. above and how one can add custom <code>css</code>, I discovered the numerous customisations one can do. Some example can be found at https://github.com/jupyter/nbconvert-examples</p> </li> </ol>","tags":["unix","vim"]},{"location":"blog/2020/11/06/-pyspark-by-example/#references-and-resources","title":"References and Resources","text":"<ul> <li>The notebooks for this post can be found at here</li> <li>SO:Calling Java/Scala function from a task</li> <li>SO:Spark performance for Scala vs Python</li> <li>PySpark Internals</li> </ul>","tags":["unix","vim"]},{"location":"blog/2020/11/06/-packaging-scala-applications/","title":"\ud83c\udf81 Packaging Scala Applications","text":"<p>Scala compiles down to Java byte code, which can then be run on any system running the JVM. It would be nice if one could extend this to native system binaries that can be run anywhere. Here I walk through the steps of getting SBT-Native-Packager to create a native binary as well as a docker image that can run my 'Hello World' application.</p> <p>For this short post, I have created a \"simpleApp\" which is a simple \"Hello, World!\" application in Scala. The example code for this post is in the standard maven directory layout, i.e:</p> <pre><code>.\n\u251c\u2500\u2500 build.sbt\n\u251c\u2500\u2500 project\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 build.properties\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 plugins.sbt\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 project\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 target\n\u251c\u2500\u2500 src\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 main\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 scala\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0     \u2514\u2500\u2500 simple\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0         \u2514\u2500\u2500 SimpleApp.scala\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 test\n\u2502\u00a0\u00a0     \u2514\u2500\u2500 scala\n\u2502\u00a0\u00a0         \u2514\u2500\u2500 simple\n\u2514\u2500\u2500 target\n</code></pre> <p>and with the <code>SimpleApp.scala</code> code looking like so:</p> <pre><code>package simple;\n\nobject SimpleApp {\ndef main(args: Array[String]): Unit = {\n  println(\"Hello, world!\")\n}\n}\n</code></pre> <p>Assuming you are familiar with <code>sbt</code>, one can use the following to compile the program to Java byte code and run:</p> <pre><code>$ cd simpleApp\n$ sbt clean compile run\n[info] Loading settings for project simpleapp-build from plugins.sbt ...\n[info] Loading project definition from /Users/tallamjr/www/blog/code/posts/2020-02-10-SBT-native-packaging/simpleApp/project\n[info] Loading settings for project simpleapp from build.sbt ...\n[info] Set current project to SimpleSBTProject (in build file:/Users/tallamjr/www/blog/code/posts/2020-02-10-SBT-native-packaging/simpleApp/)\n[info] Executing in batch mode. For better performance use sbt's shell\n[success] Total time: 0 s, completed 21-May-2020 11:16:49\n[info] Compiling 1 Scala source to /Users/tallamjr/www/blog/code/posts/2020-02-10-SBT-native-packaging/simpleApp/target/scala-2.12/classes ...\n[success] Total time: 5 s, completed 21-May-2020 11:16:54\n[info] running simple.SimpleApp\nHello, world!\n[success] Total time: 0 s, completed 21-May-2020 11:16:54\n</code></pre> <p>Alternativly:</p> <pre><code>$ sbt \"runMain simple.SimpleApp\"\n[info] Loading settings for project simpleapp-build from plugins.sbt ...\n[info] Loading project definition from /Users/tallamjr/www/blog/code/posts/2020-02-10-SBT-native-packaging/simpleApp/project\n[info] Loading settings for project simpleapp from build.sbt ...\n[info] Set current project to SimpleSBTProject (in build file:/Users/tallamjr/www/blog/code/posts/2020-02-10-SBT-native-packaging/simpleApp/)\n[info] Compiling 1 Scala source to /Users/tallamjr/www/blog/code/posts/2020-02-10-SBT-native-packaging/simpleApp/target/scala-2.12/classes ...\n[info] running simple.SimpleApp\nHello, world!\n[success] Total time: 4 s, completed 21-May-2020 11:22:34\n</code></pre> <p>Ok, great. We have a running Scala application built and run with <code>sbt</code>. But the point of this post is that we want to run a native binary...</p> <p>To achieve this <code>sbt-native-packager</code> plugin is used. Before we use it, one must ensure the plugin is \"installed\" in the <code>project/plugins.sbt</code> file like so:</p> <pre><code>$ cat -n project/plugins.sbt\n\n 1  logLevel := Level.Warn\n 2\n 3  // for autoplugins\n 4  addSbtPlugin(\"com.typesafe.sbt\" % \"sbt-native-packager\" % \"1.6.1\")\n</code></pre> <p>Furthermore we need to ensure we have set the right variables in the <code>build.sbt</code> file such as:</p> <pre><code>$ sed -n 1,7p build.sbt\n\nname := \"SimpleSBTProject\"\n\nversion := \"1.0\"\n\nscalaVersion := \"2.12.8\"\n\nenablePlugins(JavaAppPackaging)\n</code></pre> <p>With these two modified files and the project directory set up the way it is, we are ready to create our binary using the <code>stage</code> directive for <code>sbt</code>.</p> <pre><code>$ sbt clean compile stage\n\n[info] Loading settings for project simpleapp-build from plugins.sbt ...\n[info] Loading project definition from /Users/tallamjr/www/blog/code/posts/2020-02-10-SBT-native-packaging/simpleApp/project\n[info] Loading settings for project simpleapp from build.sbt ...\n[info] Set current project to SimpleSBTProject (in build file:/Users/tallamjr/www/blog/code/posts/2020-02-10-SBT-native-packaging/simpleApp/)\n[info] Executing in batch mode. For better performance use sbt's shell\n[success] Total time: 0 s, completed 21-May-2020 11:29:50\n[info] Compiling 1 Scala source to /Users/tallamjr/www/blog/code/posts/2020-02-10-SBT-native-packaging/simpleApp/target/scala-2.12/classes ...\n[success] Total time: 5 s, completed 21-May-2020 11:29:55\n[info] Main Scala API documentation to /Users/tallamjr/www/blog/code/posts/2020-02-10-SBT-native-packaging/simpleApp/target/scala-2.12/api...\n[info] Wrote /Users/tallamjr/www/blog/code/posts/2020-02-10-SBT-native-packaging/simpleApp/target/scala-2.12/simplesbtproject_2.12-1.0.pom\nmodel contains 3 documentable templates\n[info] Main Scala API documentation successful.\n[success] Total time: 2 s, completed 21-May-2020 11:29:56\n</code></pre> <p>Using <code>stage</code> creates a new set of files, found in the <code>target/universal</code> directory:</p> <pre><code>$ tree target/universal/\ntarget/universal/\n\u251c\u2500\u2500 scripts\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 bin\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 simplesbtproject\n\u2502\u00a0\u00a0     \u2514\u2500\u2500 simplesbtproject.bat\n\u2514\u2500\u2500 stage\n    \u251c\u2500\u2500 bin\n    \u2502\u00a0\u00a0 \u251c\u2500\u2500 simplesbtproject\n    \u2502\u00a0\u00a0 \u2514\u2500\u2500 simplesbtproject.bat\n    \u2514\u2500\u2500 lib\n        \u251c\u2500\u2500 org.scala-lang.scala-library-2.12.8.jar\n        \u2514\u2500\u2500 simplesbtproject.simplesbtproject-1.0.jar\n\n5 directories, 6 files\n</code></pre> <p>Notice it creates a Unix and Windows version.</p> <p>Now let's run it...</p> <pre><code>$ ./target/universal/stage/bin/simplesbtproject\n\nHello, world!\n</code></pre> <p>Success, we can run a single \"binary\" for our application. I put binary in quotation marks there because it is not exactly a binary file that is created, and one would still need Java installed on the system, but it allows a user to run a command like the above which is better suited for production settings.</p> <p>Ok, now that we can do this, \"What about making a Docker image!\" I hear you cry. This can also be done with some minor additions to the <code>build.sbt</code> file with:</p> <pre><code>$ sed -n 9,18p build.sbt\n\nenablePlugins(DockerPlugin)\n\n// change the name of the project adding the prefix of the user\npackageName in Docker := \"tallamjr/\" +  packageName.value\n//the base docker images\ndockerBaseImage := \"java:8-jre\"\n//the exposed port\ndockerExposedPorts := Seq(9000)\n//exposed volumes\ndockerExposedVolumes := Seq(\"/opt/docker/logs\")\n</code></pre> <p>With this in place, we can use <code>publishLocal</code> to create a docker image locally, which, when built we will run.</p> <p>Just to convince ourselves a new docker images is created <code>docker images</code> reveals only other projects after doing a <code>docker system prune -a</code>:</p> <pre><code>$ docker images\n\nREPOSITORY          TAG                 IMAGE ID            CREATED             SIZE\nmachcoll_bluebird   latest              140ca65afae2        2 months ago        1.15GB\nmachcoll-img        dev-1bea126         38298723afa1        2 months ago        1.01GB\nnats-metrics-img    master-9a39559      3af43d2f4bd7        2 months ago        1.1GB\n</code></pre> <p>Running <code>sbt publish:Local</code> creates the docker image for us (I have excluded much of the output for brevity)</p> <pre><code>$ sbt publish:Local\n...\n[info] Built image tallamjr/simpleapp with tags [1.0]\n[success] Total time: 53 s, completed 21-May-2020 11:46:06\n</code></pre> <p>Now when we do <code>docker images</code> again we should see our new docker image:</p> <pre><code>$ docker images\n\nREPOSITORY           TAG                 IMAGE ID            CREATED              SIZE\ntallamjr/simpleapp   1.0                 6eb32a8b2939        About a minute ago   317MB\nmachcoll_bluebird    latest              140ca65afae2        2 months ago         1.15GB\nmachcoll-img         dev-1bea126         38298723afa1        2 months ago         1.01GB\nnats-metrics-img     master-9a39559      3af43d2f4bd7        2 months ago         1.1GB\njava                 8-jre               e44d62cf8862        3 years ago          311MB\n</code></pre> <p>Let's test this:</p> <pre><code>$ docker run tallamjr/simpleapp:1.0\n\nHello, world!\n</code></pre> <p>So there we have it! <code>sbt-native-packager</code> is able to also publish to a docker registry and in addition to standard scripts like the one above, one can also create other formats</p> <pre><code>universal:packageBin - Generates a universal zip file\nuniversal:packageZipTarball - Generates a universal tgz file\ndebian:packageBin - Generates a deb\ndocker:publishLocal - Builds a Docker image using the local Docker server\nrpm:packageBin - Generates an rpm\nuniversal:packageOsxDmg - Generates a DMG file with the same contents as the universal zip/tgz.\nwindows:packageBin - Generates an MSI\n</code></pre> <p>For more check out the great documentation here</p> <p>All the code for this post is available on Github</p>","tags":["unix","vim"]},{"location":"blog/2020/11/06/-calling-compiled-scala-code-from-python-using-pyspark/","title":"\ud83d\udc0d Calling Compiled Scala Code from Python using PySpark","text":"<p>Calling compiled Scala code inside the JVM from Python using PySpark.</p> <p>There is no doubt that Java and Scala are the de-facto languages for Data Engineering, whilst Python is certainly the front runner for language of choice with Data Scientists. Spark; a framework for distributed data analytics is written in Scala but allows for usage in Python, R and Java. Interoperability between Java and Scala is a no briner since Scala compiles down to Java byte code, but call Scala from Python is a little more involved, but the process is very simple.</p> <p>The code used in this post builds upon the code used in a previous post and has the standard maven directory layout. To have a closer look, it can be found under <code>code/posts/2020-05-20-Scala-Python-JVM</code></p> <p>We will be calling <code>simple.SimpleApp.hello()</code> function to print <code>\"Hello, World!\"</code>.</p> <p>The simple Scala we will use is the following:</p> src/main/scala/simple/SimpleApp.scala<pre><code>package simple;\n\nobject SimpleApp {\n  def hello(): Unit = {\n    println(\"Hello, Wolrd\")\n  }\n}\n</code></pre> <p>This will then be compiled and packaged using <code>sbt</code> to created a <code>.jar</code> file that can be included in the running JVM instance when launching Spark. Thus, after running:</p> <pre><code>$ sbt clean compile package\n\n[info] Loading settings for project simpleapp-build from plugins.sbt ...\n[info] Loading project definition from /Users/tallamjr/www/blog/code/posts/2020-05-20-Scala-Python-JVM/simpleApp/project\n[info] Loading settings for project simpleapp from build.sbt ...\n[info] Set current project to SimpleApp (in build file:/Users/tallamjr/www/blog/code/posts/2020-05-20-Scala-Python-JVM/simpleApp/)\n[info] Executing in batch mode. For better performance use sbt's shell\n[success] Total time: 0 s, completed 21-May-2020 13:18:19\n[info] Compiling 1 Scala source to /Users/tallamjr/www/blog/code/posts/2020-05-20-Scala-Python-JVM/simpleApp/target/scala-2.12/classes ...\n[success] Total time: 7 s, completed 21-May-2020 13:18:25\n[success] Total time: 0 s, completed 21-May-2020 13:18:26\n</code></pre> <p>We obtain <code>target/scala-2.12/simpleapp_2.12-1.0.jar</code> which is supplied to Spark like so:</p> <pre><code>$ spark-submit --driver-class-path target/scala-2.12/simpleapp_2.12-1.0.jar simpleSpark/main.py\n</code></pre> <p><code>simpleSpark/main.py</code> is the where the <code>pyspark</code> code lives that will be calling the Scala function, let's have a look into that file:</p> <pre><code>from pyspark.sql import SparkSession\n\nspark = SparkSession.builder \\\n    .master(\"local[*]\") \\\n    .appName(\"simpleSpark\") \\\n    .getOrCreate()\n\nsc = spark.sparkContext\nsc.setLogLevel(\"ERROR\")\n\n\ndef logLevel(sc):\n    # REF: https://stackoverflow.com/questions/25193488/how-to-turn-off-info-logging-in-spark\n    log4jLogger = sc._jvm.org.apache.log4j\n    log4jLogger.Logger.getLogger(\"org\").setLevel(log4jLogger.Level.ERROR)\n    log = log4jLogger.LogManager.getLogger(__name__)\n    log.warn(\"Custom WARN message\")\n\n\nlogLevel(spark)\n\n\nprint(spark.range(5000).where(\"id &gt; 500\").selectExpr(\"sum(id)\").collect())\n\nsc._jvm.simple.SimpleApp.hello()\n</code></pre> <p>There is a bit of boilerplate to get the session started and some customisation with logging going on but the key lines of code are:</p> <pre><code>...\n 8  sc = spark.sparkContext\n...\n25  sc._jvm.simple.SimpleApp.hello()\n</code></pre> <p>The resulting output after running <code>spark-submit</code> is:</p> <pre><code>$ spark-submit --driver-class-path target/scala-2.12/simpleapp_2.12-1.0.jar simpleSpark/main.py\n\n20/05/21 13:26:02 WARN Utils: Your hostname, Tareks-MacBook-Pro.local resolves to a loopback address: 127.0.0.1; using 192.168.0.178 instead (on interface en0)\n20/05/21 13:26:02 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n20/05/21 13:26:02 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n[Row(sum(id)=12372250)]\nHello, Wolrd!\n</code></pre>","tags":["unix","vim"]},{"location":"blog/2020/11/06/-calling-compiled-scala-code-from-python-using-pyspark/#references","title":"References","text":"<p>This post was inspired by Alexis Seigneurin's much more detailed post Spark - Calling Scala code from PySpark which I highly recommend reading.</p>","tags":["unix","vim"]},{"location":"blog/2024/06/17/-maintaining-code-quality-with-pre-commit/","title":"\ud83e\udde9 Maintaining Code Quality with <code>pre-commit</code>","text":"<p>Some tips and tools to help keep research software reproducible, reliable and robust.</p> Code quality.. <p>In the realm of research software engineering, writing clean, maintainable, and reproducible code is essential. Researchers often develop software that is shared among teams, published at conferences or journals, or used to derive critical results in adjacent fields. Adhering to best practices ensures that your code is not only reliable but understandable and reproducible. This post will hopefully give some motivation with examples and showcase several technologies that can help make putting the best practises into practice easier.</p> <p>The post will focus on using <code>pre-commit</code> for maintaining (or enforcing, depending how you look at it) code quality, while other posts that will follow will cover <code>pytest</code> for testing, and <code>git</code> for version control.</p>","tags":["git"]},{"location":"blog/2024/06/17/-maintaining-code-quality-with-pre-commit/#why-best-practices-matter","title":"\u2753Why Best Practices Matter","text":"<ol> <li>Reproducibility: Ensures that research findings can be consistently replicated.</li> <li>Collaboration: Makes it easier for team members to understand and contribute to the project.</li> <li>Maintenance: Reduces the technical debt and makes the code easier to maintain and extend.</li> <li>Reliability: Helps in catching bugs and issues early, ensuring robust software.</li> </ol>","tags":["git"]},{"location":"blog/2024/06/17/-maintaining-code-quality-with-pre-commit/#code-quality-with-pre-commit","title":"\u2705 Code Quality with <code>pre-commit</code>","text":"<p>Maintaining code quality is critical, and the <code>pre-commit</code> framework helps automate this by running checks before code is committed. While some may seen code style or quality a non-issue, having a consistent style or checking for linting errors can relive the mental burden of the programmer significantly. It is also amazingly easy to automate so why not get tools in place so you don't need to even think about it.</p> <p>While other frameworks are out there such as Husky<sup>1</sup> and Overcommit<sup>2</sup> <code>pre-commit</code> allows developers to define a set of checks (hooks) that run before a commit is made, ensuring code quality and consistency before it enters the repository. These hooks can perform tasks such as code formatting, linting, checking for secrets, or running tests. Originally created by Yelp in 2014, the goal was to provide a common interface for pre-commit hooks across different languages and projects, promoting best practices and code quality from the very start of the development process.</p>","tags":["git"]},{"location":"blog/2024/06/17/-maintaining-code-quality-with-pre-commit/#getting-started-with-pre-commit","title":"Getting Started with pre-commit","text":"<p>1. Installation:</p> <p>The easiest way to install is via <code>pip</code>, but other options available too such as <code>conda</code> etc. Let's put together a minimal repo and install our first pre-commit checks.</p> Info <p>For this demo example I will use other additional technologies but feel free   to use whichever you are familiar with. This example will feature Hatch.</p> <pre><code>$ hatch new hello\nhello\n\u251c\u2500\u2500 src\n\u2502   \u2514\u2500\u2500 hello\n\u2502       \u251c\u2500\u2500 __about__.py\n\u2502       \u2514\u2500\u2500 __init__.py\n\u251c\u2500\u2500 tests\n\u2502   \u2514\u2500\u2500 __init__.py\n\u251c\u2500\u2500 LICENSE.txt\n\u251c\u2500\u2500 README.md\n\u2514\u2500\u2500 pyproject.toml\n\u2714 /tmp/hello (master) :: echo 'print(\"Hello, World!\")' &gt;&gt; src/hello/main.py\n</code></pre> <p>2. Configuration:</p> <p>When it is installed, one simply creates a <code>.pre-commit-config.yml</code> file in the root of the repo like so:</p> <pre><code># .pre-commit-config.yml\nrepos:\n  - repo: https://github.com/pre-commit/pre-commit-hooks\n    rev: v4.0.1\n    hooks:\n      - id: trailing-whitespace\n      - id: end-of-file-fixer\n      - id: check-yaml\n      - id: check-added-large-files\n  - repo: https://github.com/psf/black\n    rev: 24.4.2\n    hooks:\n      - id: black\n  - repo: https://github.com/astral-sh/ruff-pre-commit\n    rev: v0.5.5\n    hooks:\n      - id: ruff\n</code></pre> <p>Don't worry, we will cover the hooks later on...</p> <p>3. Install pre-commit Hooks:</p> <pre><code>$ pre-commit install\n</code></pre> <p>Note</p> <p>This will only install the necessary hooks locally. It is important to stress that these do not take affect upstream even after a push. So it is encouraged in team development there are guidelines about setting up pre-commit.</p> Warning <p>pre-commit naturally requires there to be a version control system in place to work. Otherwise you see the following error: <code>console     $ pre-commit install     An error has occurred: FatalError: git failed. Is it installed, and are you     in a Git repository directory?  Check the log at     /Users/tallam/.cache/pre-commit/pre-commit.log</code></p> <pre><code>$ pre-commit install\npre-commit installed at .git/hooks/pre-commit\n</code></pre> <ol> <li>Run pre-commit:</li> </ol> <p>You can manually run all hooks on all files:</p> <pre><code>$ pre-commit run --all-files\n</code></pre> <pre><code>$ pre-commit run --all-files\n[INFO] Initializing environment for https://github.com/astral-sh/ruff-pre-commit.\n[INFO] Installing environment for https://github.com/psf/black.\n[INFO] Once installed this environment will be reused.\n[INFO] This may take a few minutes...\n[INFO] Installing environment for https://github.com/astral-sh/ruff-pre-commit.\n[INFO] Once installed this environment will be reused.\n[INFO] This may take a few minutes...\nTrim Trailing Whitespace.................................................Passed\nFix End of Files.........................................................Passed\nCheck Yaml...............................................................Passed\nCheck for added large files..............................................Passed\nblack....................................................................Passed\nruff.....................................................................Passed\n</code></pre> <p>You may have noticed above that pre-commit has some built in checkers that are very useful and have been highlighted below.</p> <pre><code># .pre-commit-config.yml\nrepos:\n  - repo: https://github.com/pre-commit/pre-commit-hooks\n    rev: v4.0.1\n    hooks:\n      - id: trailing-whitespace\n      - id: end-of-file-fixer\n      - id: check-yaml\n      - id: check-added-large-files\n  - repo: https://github.com/psf/black\n    rev: 24.4.2\n    hooks:\n      - id: black\n  - repo: https://github.com/astral-sh/ruff-pre-commit\n    rev: v0.5.5\n    hooks:\n      - id: ruff\n</code></pre> <p>One simple example is <code>trailing-whitespace</code> which will fail on extra white space. Let's add some to the <code>main.py</code> file to see what happens.</p> <pre><code>$ echo \"     \" &gt;&gt; src/hello/main.py\n</code></pre> <pre><code>diff --git a/src/hello/main.py b/src/hello/main.py\nindex 7df869a..3c8ba12 100644\n--- a/src/hello/main.py\n+++ b/src/hello/main.py\n@@ -1 +1,2 @@\n print(\"Hello, World!\")\n+\n</code></pre> <pre><code>$ git add . &amp;&amp; git commit -m \"whitespace test\"\nTrim Trailing Whitespace.................................................Failed\n- hook id: trailing-whitespace\n- exit code: 1\n- files were modified by this hook\n\nFixing src/hello/main.py\n\nFix End of Files.........................................................Failed\n- hook id: end-of-file-fixer\n- exit code: 1\n- files were modified by this hook\n\nFixing src/hello/main.py\n\nCheck Yaml...........................................(no files to check)Skipped\nCheck for added large files..............................................Passed\nblack....................................................................Passed\nruff.....................................................................Passed\n</code></pre> <p>Fail. But pre-commit can automatically apply the fix for you. So when we have another look we see it has already been removed:</p> <pre><code>$ git status\nOn branch master\nChanges to be committed:\n  (use \"git restore --staged &lt;file&gt;...\" to unstage)\n        modified:   src/hello/main.py\n\nChanges not staged for commit:\n  (use \"git add &lt;file&gt;...\" to update what will be committed)\n  (use \"git restore &lt;file&gt;...\" to discard changes in working directory)\n        modified:   src/hello/main.py\n</code></pre> <pre><code># git diff\ndiff --git a/src/hello/main.py b/src/hello/main.py\nindex 3c8ba12..7df869a 100644\n--- a/src/hello/main.py\n+++ b/src/hello/main.py\n@@ -1,2 +1 @@\n print(\"Hello, World!\")\n-\n</code></pre> <pre><code># git diff --staged\ndiff --git a/src/hello/main.py b/src/hello/main.py\nindex 7df869a..3c8ba12 100644\n--- a/src/hello/main.py\n+++ b/src/hello/main.py\n@@ -1 +1,2 @@\n print(\"Hello, World!\")\n+\n</code></pre> <p>This may seem annoying, \"it's only whitespace...\ud83d\ude44\", but as you can see above, having the slightest change to what is expected can cause unnecessary diffs in the git logs. This means it makes it that much harder for your team members trying to get their head around complex code changes when there are random changes that do not change the logic of the code and just becomes a distraction. And trust me, when combing through 1000's of lines of code looking for a bug, the less distractions and diff content the better!</p> <p>As programmers we spend way more time reading code that writing it and so enforcing code quality not only applies to silly examples like whitespace, but even the consistency of indentation, or style of <code>if;else</code> blocks to name a few. I'd encourage at this point to look at the built in hooks found here: https://pre-commit.com/hooks.html</p> <p>All can make a difference and having a strict enforcement of style will make reading code far easier.</p> <p>Some particularly useful ones are <code>black</code>: the \"opinionated python formatter\", <code>ruff</code>: a blazingly fast linter for python and <code>mypy</code> for pseudo-type checking of python code.</p>","tags":["git"]},{"location":"blog/2024/06/17/-maintaining-code-quality-with-pre-commit/#custom-hooks","title":"Custom Hooks","text":"<p>The awesome thing about pre-commit is you can also create custom hooks to enforce specific project requirements.</p> <pre><code># .pre-commit-config.yaml\nrepos:\n  - repo: local\n    hooks:\n      - id: custom-check\n        name: Custom Check\n        entry: ./scripts/custom-check.sh\n        language: script\n        files: \\.py$\n</code></pre> ./scripts/custom-check.sh<pre><code>#!/bin/bash\n# Custom script for checking something specific\necho \"Running custom check\"\n</code></pre>","tags":["git"]},{"location":"blog/2024/06/17/-maintaining-code-quality-with-pre-commit/#conclusion","title":"Conclusion","text":"<p>Hopefully this has given you some motivation and also pointed you in the direction where you can learn more about enforcing code quality in your projects. Always strive to automate everything and if you can automate code quality checks, why not?!</p> <p>Happy coding! And be forceful with style</p> <ol> <li> <p>https://typicode.github.io/husky/\u00a0\u21a9</p> </li> <li> <p>https://github.com/sds/overcommit\u00a0\u21a9</p> </li> </ol>","tags":["git"]},{"location":"blog/templates/template/","title":"Template post","text":"<p>This is the story about a template post</p> <p>!(tallamjr?)(tallamjr? avatar)</p> <p>Tarek Allam Jr.\u00b7 @tallamjr  September 13, 2021 \u00b7  15 min read \u00b7 </p> <p>(tallamjr? avatar): https://avatars.githubusercontent.com/tallamjr</p> <p> </p> <p></p> <p>I'm a figure caption...</p> <ul> <li>Scala and SBT Introduction</li> <li>SBT-Native-Packager</li> <li>Docker</li> </ul> <pre><code>print(f\"Numpy: {np.__version__}\")\n</code></pre> <p>Say if I said something here</p> <pre><code>$ echo \"Hello World!\"\n</code></pre> <pre><code>println(\"hello\")\ndef somefunction(col: String)\n\nval mate = Int 5\n</code></pre>"},{"location":"blog/templates/template/#native","title":"Native","text":""},{"location":"blog/templates/template/#docker","title":"Docker","text":"<ul> <li>Apache Arrow</li> <li>Databases</li> <li>Spark 3.0, Horovod, Plotly</li> <li>Embedded Systems</li> <li>Rust</li> <li>TensorFlow 2.x</li> <li>Compression</li> <li>Codecs</li> </ul> <p>\\(\\sqrt{3x-1}+(1+x)^2\\)</p>"},{"location":"blog/templates/template/#maxwells-equations","title":"Maxwell's Equations","text":"equation description \\(\\nabla \\cdot \\vec{\\mathbf{B}}  = 0\\) divergence of \\(\\vec{\\mathbf{B}}\\) is zero \\(\\nabla \\times \\vec{\\mathbf{E}}\\, +\\, \\frac1c\\, \\frac{\\partial\\vec{\\mathbf{B}}}{\\partial t}  = \\vec{\\mathbf{0}}\\) curl of \\(\\vec{\\mathbf{E}}\\) is proportional to the rate of change of \\(\\vec{\\mathbf{B}}\\) \\(\\nabla \\times \\vec{\\mathbf{B}} -\\, \\frac1c\\, \\frac{\\partial\\vec{\\mathbf{E}}}{\\partial t} = \\frac{4\\pi}{c}\\vec{\\mathbf{j}}    \\nabla \\cdot \\vec{\\mathbf{E}} = 4 \\pi \\rho\\) wha? \\[ \\begin{equation}   g\\left(k\\right) = \\binom{n}{k} p^k\\left(1-p\\right)^{n-k} \\end{equation} \\] \\[ \\begin{equation} f(x) = \\int_{-\\infty}^\\infty \\hat f(\\xi)\\,e^{2 \\pi i \\xi x} \\,d\\xi \\end{equation} \\] \\[f(x) = \\int_{-\\infty}^\\infty \\hat f(\\xi)\\,e^{2 \\pi i \\xi x} \\,d\\xi\\] <p>In equation \\(\\eqref{eq:sampleref}\\), we find the value of an interesting integral:</p> \\[ \\begin{equation}   \\int_0^\\infty \\frac{x^3}{e^x-1}\\,dx = \\frac{\\pi^4}{15}   \\label{eq:sampleref} \\end{equation} \\] <ul> <li>Scala and SBT Introduction</li> <li>SBT-Native-Packager</li> <li>Docker</li> </ul> <pre><code>print(f\"Numpy: {np.__version__}\")\n</code></pre> <p>Say if I said something here</p> <pre><code>$ echo \"Hello World!\"\n</code></pre> <pre><code>println(\"hello\")\ndef somefunction(col: String)\n\nval mate = Int 5\n</code></pre>"},{"location":"blog/templates/template/#matlab","title":"MATLAB","text":""},{"location":"projects/","title":"Recent Projects","text":"<p>Below is a collection of open-source personal projects I am working on, as well as some very interesting projects I am lucky enough to be involved with. If you like the work here and would like to support more from me, you can help sponsor contributions here </p>"},{"location":"projects/#simurgh","title":"simurgh","text":"<p><code>simurgh</code> (pronounced Seymour) is an open source platform that supports developing and evaluating algorithms (AI agents) for automated air traffic control. It provides an easy to use interface for running experiments in an air traffic simulator as well as packages that support agent development.</p> <p>Air traffic control (ATC) is a complex task requiring real-time safety-critical decision making. In practice, air traffic control operators (ATCOs) monitor a given sector and issue commands to aircraft pilots to ensure safe separation between aircraft. They also have to consider the number and frequency of instructions issued, fuel efficiency and orderly handover between sectors. Optimising for the multiple objectives while accounting for uncertainty (e.g., due to aircraft mass, pilot behaviour or weather conditions) makes this a particularly complex task.</p> <p>The Simurgh project provides a research-focused user-friendly platform for testing automated approaches to ATC</p>"},{"location":"projects/#wwwastroinformaticsxyz","title":"www.astroinformatics.xyz","text":"<p><code>astroinformatics.xyz</code> is a online book inspired by the OpenAI book SpinningUp. The idea is to have a resource that outlines the mathematics and technology involved with doing Astroinformatics and the Mathematics and Data Science that underpins the research. It is still very much a work in progress, but the plan is to have 3 main sections: 1. Data Science for the Mathematics and Machine Learning theory. 2. Data Engineering; to showcase the technologies used, like Apache Spark or Apache Kafka for instance. 3. Research Engineering to outline the tools of the trade for reproducible research, model deployment and production system design.</p>"},{"location":"projects/#option3","title":"option3","text":"<p><code>option3</code> is a proof of concept application that is being used to improve my understanding of Kafka and Spark for developing machine learning data pipelines. Inspired by Stephane Maarek's Kafka for Beginners course, I hope to connect to the Twitter stream of tweets, apply some filtering and transformations, and finally visualise, in real-time, the processed data.</p> <p></p> <p>Follow me on Github and check out more of my projects.</p> <p> </p> <p></p>"},{"location":"wiki/compilers/","title":"Compilers","text":"<p>\ud83d\udc32 Here be Dragons!</p>"},{"location":"wiki/compilers/#cross-compilers","title":"Cross-compilers","text":""},{"location":"wiki/compilers/#ml-compilers","title":"ML Compilers","text":"<p>MATCH Abstracts Away Hardware Differences to Deliver Better TinyML on a Range of Microcontrollers https://arxiv.org/pdf/2410.08855</p>"},{"location":"wiki/compression/","title":"\ud83d\udddc\ufe0f Compression","text":""},{"location":"wiki/compression/#stanford","title":"Stanford","text":"<ul> <li>https://stanforddatacompressionclass.github.io/notes/contents.html</li> <li>https://www.youtube.com/playlist?list=PLoROMvodv4rPj4uhbgUAaEKwNNak8xgkz</li> <li>https://stanforddatacompressionclass.github.io/Fall23/lectures/</li> </ul>"},{"location":"wiki/compression/#balmer-course","title":"Balmer Course","text":"<ul> <li>https://robamler.github.io/teaching/compress21/</li> <li>https://www.youtube.com/playlist?list=PL05umP7R6ij0Mp1dW2HuXlb-UQIYnv8xK</li> <li>https://arxiv.org/pdf/2201.01741</li> <li>https://bamler-lab.github.io/constriction/</li> </ul>"},{"location":"wiki/compression/#understanding-compression-book","title":"Understanding Compression Book","text":"<p>LINK</p>"},{"location":"wiki/compression/#hutter-prize","title":"Hutter Prize","text":"<ul> <li>http://prize.hutter1.net/index.htm</li> <li>http://prize.hutter1.net/hfaq.htm#start</li> </ul>"},{"location":"wiki/compression/data-compression/","title":"Data Compression","text":""},{"location":"wiki/compression/data-compression/#sparse-representations","title":"Sparse Representations","text":""},{"location":"wiki/compression/data-compression/#lossless-compression","title":"Lossless Compression","text":""},{"location":"wiki/compression/model-compression/","title":"Model Compression","text":""},{"location":"wiki/compression/model-compression/#quantization","title":"Quantization","text":""},{"location":"wiki/compression/model-compression/#weight-pruning","title":"Weight Pruning","text":""},{"location":"wiki/compression/model-compression/#weight-clustering","title":"Weight Clustering","text":""},{"location":"wiki/compression/lossy/lossy/","title":"Lossy Compression","text":"<p>Lossy compression aims to reduce file sizes by removing some information that is less perceptible to the human eye (or ear), trading off perfect reconstruction for higher compression ratios. In image and video media, various techniques are used that typically follow a common pipeline:</p> <ol> <li>Transform Coding: Convert spatial (or temporal) data into a domain where energy is compacted into a few coefficients.</li> <li>Quantization: Reduce precision of transformed coefficients, discarding less important information.</li> <li>Entropy Coding: Apply lossless coding (e.g., Huffman or arithmetic coding) to the quantized coefficients.</li> </ol>"},{"location":"wiki/compression/lossy/lossy/#1-historical-background","title":"1. Historical Background","text":"<ul> <li> <p>Early Developments:   The concept of lossy compression has its roots in information theory. Claude Shannon\u2019s work in the 1940s laid the theoretical groundwork by describing the limits of data compression. In the 1970s, researchers began exploring practical transforms, with the Discrete Cosine Transform (DCT) emerging as a powerful tool.</p> </li> <li> <p>Nasir Ahmed et al. (1974): Pioneered the DCT, which quickly became the foundation for many image compression schemes.</p> </li> <li> <p>JPEG Standard:</p> </li> <li> <p>Late 1980s\u2013Early 1990s: The Joint Photographic Experts Group (JPEG) standardized a compression method based on block-based DCT. The first JPEG standard (ISO/IEC 10918-1) was published in 1992.</p> </li> <li> <p>The JPEG standard was widely adopted for digital photography and web images, thanks to its effective balance between image quality and file size.</p> </li> <li> <p>Wavelet Compression:</p> </li> <li> <p>JPEG2000 (2000): Introduced wavelet transforms to image compression, offering features such as lossless and lossy compression, scalability, and region-of-interest (ROI) coding. The Discrete Wavelet Transform (DWT) replaced the block-based DCT in many aspects, addressing issues like block artifacts common in JPEG.</p> </li> <li> <p>Video Compression Evolution:</p> </li> <li>MPEG Standards:<ul> <li>MPEG-1 (1993): Primarily targeted at VHS-quality video, it extended image compression techniques (like the DCT) to handle temporal redundancy by using inter-frame prediction.</li> <li>MPEG-2 (mid-1990s): Became the basis for digital television and DVD video, improving on motion compensation and coding efficiency.</li> </ul> </li> <li>Modern Codecs:<ul> <li>H.264/AVC (2003): Introduced advanced techniques such as variable block-size motion compensation and more efficient entropy coding (e.g., CABAC, CAVLC).</li> <li>HEVC (H.265, 2013): Further refined coding efficiency with larger and more flexible block structures (quad-tree partitions) and enhanced motion compensation, enabling high-resolution video compression.</li> </ul> </li> </ul>"},{"location":"wiki/compression/lossy/lossy/#2-detailed-technical-components","title":"2. Detailed Technical Components","text":""},{"location":"wiki/compression/lossy/lossy/#a-transform-coding","title":"A. Transform Coding","text":"<p>Discrete Cosine Transform (DCT): The DCT concentrates signal energy into a few coefficients, making it easier to discard less important information. For an 8\u00d78 block, the forward DCT is:</p> \\[ F(u,v) = \\frac{1}{4} C(u) C(v) \\sum*{x=0}^{7}\\sum*{y=0}^{7} f(x,y) \\cos\\!\\left[\\frac{(2x+1)u\\pi}{16}\\right] \\cos\\!\\left[\\frac{(2y+1)v\\pi}{16}\\right], \\] <p>with scaling factors:</p> \\[ C(u) = \\begin{cases} \\frac{1}{\\sqrt{2}} &amp; \\text{if } u=0, \\\\ 1 &amp; \\text{if } u&gt;0, \\end{cases} \\quad C(v) = \\begin{cases} \\frac{1}{\\sqrt{2}} &amp; \\text{if } v=0, \\\\ 1 &amp; \\text{if } v&gt;0. \\end{cases} \\] <p>The inverse DCT (IDCT) reconstructs the image block:</p> \\[ f(x,y) = \\frac{1}{4} \\sum*{u=0}^{7}\\sum*{v=0}^{7} C(u) C(v) F(u,v) \\cos\\!\\left[\\frac{(2x+1)u\\pi}{16}\\right] \\cos\\!\\left[\\frac{(2y+1)v\\pi}{16}\\right]. \\] <p>Wavelet Transform (DWT): Wavelets offer multi-resolution analysis. In a one-dimensional setting:</p> \\[ W(j, k) = \\sum*{n} f(n) \\, \\psi*{j,k}(n), \\] <p>where \\( \\psi\\_{j,k}(n) \\) is a wavelet function at scale \\( j \\) and translation \\( k \\). In two dimensions, separable filters are applied along both dimensions. Wavelet-based methods allow progressive transmission and ROI coding, which were key advantages in JPEG2000.</p>"},{"location":"wiki/compression/lossy/lossy/#b-quantization","title":"B. Quantization","text":"<p>Quantization reduces the precision of transform coefficients. For the DCT, this is commonly performed as:</p> \\[ F_q(u,v) = \\operatorname{round}\\!\\left(\\frac{F(u,v)}{Q(u,v)}\\right), \\] <p>where \\( Q(u,v) \\) is a quantization step size for each frequency component. Larger \\( Q(u,v) \\) values yield greater compression (and more loss).</p> <ul> <li>Historical Note: Early implementations of JPEG used fixed quantization matrices designed through psychovisual experiments to determine which frequencies the human eye is less sensitive to.</li> </ul> <p>For wavelet coefficients, quantization can be scalar (treating each coefficient independently) or vector-based, and often involves more sophisticated strategies to control artifacts across scales.</p>"},{"location":"wiki/compression/lossy/lossy/#c-entropy-coding","title":"C. Entropy Coding","text":"<p>After quantization, entropy coding removes statistical redundancy. Common methods include:</p> <ul> <li>Huffman Coding: Constructs a binary tree based on the probability of occurrence of quantized symbols.</li> <li>Arithmetic Coding: Represents sequences of symbols as intervals on the number line.</li> <li>Context-Adaptive Binary Arithmetic Coding (CABAC): Used in H.264/AVC and HEVC for improved efficiency by adapting probabilities based on context.</li> </ul> <p>These techniques were refined over decades, with significant contributions from research in the 1970s and 1980s.</p>"},{"location":"wiki/compression/lossy/lossy/#d-motion-estimation-and-compensation-in-video","title":"D. Motion Estimation and Compensation in Video","text":"<p>To exploit temporal redundancy, video codecs use motion estimation (finding similar blocks in consecutive frames) and motion compensation (predicting the current frame from previous frames). For a block in frame \\( t \\):</p> <ol> <li>Prediction:</li> </ol> <p>[    \\hat{f}t(x,y) = f(x + \\Delta x, y + \\Delta y),    ]</p> <p>where \\( \\Delta x \\) and \\( \\Delta y \\) are motion vector components.</p> <ol> <li>Residual Calculation:</li> </ol> <p>[    r(x,y) = f_t(x,y) - \\hat{f}_t(x,y).    ]</p> <p>The residual is then processed using transform coding (typically DCT) and quantization. This approach was a cornerstone of MPEG-\u00bd and later advanced in H.264 and HEVC.</p>"},{"location":"wiki/compression/lossy/lossy/#e-rate-distortion-optimization-rdo","title":"E. Rate-Distortion Optimization (RDO)","text":"<p>Balancing compression rate and quality is formalized via rate-distortion optimization. The cost function is often expressed as:</p> \\[ J = D + \\lambda R, \\] <p>where:</p> <ul> <li>\\( D \\) is the distortion (e.g., Mean Squared Error between the original and reconstructed signal),</li> <li>\\( R \\) is the rate (number of bits),</li> <li>\\( \\lambda \\) is a Lagrange multiplier balancing the trade-off.</li> </ul> <p>RDO has been a central concept since the late 1980s and early 1990s in codec design, allowing algorithms to dynamically select coding parameters (like quantization step sizes or block partitions) to achieve the best quality for a given bit rate.</p>"},{"location":"wiki/compression/lossy/lossy/#3-comparison-of-techniques-with-historical-insights","title":"3. Comparison of Techniques with Historical Insights","text":"Technique Domain Main Transform Quantization Strategy Key Additional Features Historical Context &amp; References JPEG Image Block-based 2D DCT Fixed/adaptive quantization matrix Chroma subsampling, zigzag scan, run-length and Huffman coding Standardized in 1992; based on research from the 1970s (Nasir Ahmed et al.). Widely adopted for web and digital photography. [Wallace, G. K., 1991] JPEG2000 Image Discrete Wavelet Transform (DWT) Scalar or embedded quantization Scalability, ROI coding, error resilience (EBCOT entropy coding) Published in 2000; developed to overcome JPEG\u2019s limitations (e.g., block artifacts). Based on wavelet research from the 1980s\u20131990s. [Taubman &amp; Marcellin, 2002] MPEG-\u00bd Video Block-based 2D DCT (per frame) Adaptive quantization with variable block sizes Motion estimation and compensation; Group-of-Pictures (GOP) structure; simple motion vectors MPEG-1 (1993) and MPEG-2 (mid-1990s) built on image compression (like JPEG) and extended them for video, integrating temporal redundancy. [ISO/IEC standards for MPEG-1 and MPEG-2] H.264/AVC Video Block-based DCT/Integer transforms Rate-distortion optimized quantization Intra-/inter-frame prediction, variable block sizes (macroblocks, partitions), advanced entropy coding (CABAC, CAVLC) Standardized in 2003; marked a major leap in efficiency and flexibility. Introduced sophisticated RDO and improved motion compensation, influenced by earlier MPEG designs. [ITU-T Recommendation H.264] HEVC (H.265) Video Variable block-size integer transforms Advanced rate-distortion quantization Quad-tree coding units, improved motion compensation, better performance at high resolutions, and more efficient parallel processing Standardized in 2013; developed to address the increasing demand for high-resolution video (e.g., 4K, 8K). Built on lessons from H.264 with new partitioning schemes and prediction modes."},{"location":"wiki/compression/lossy/lossy/#4-further-developments-and-research-directions","title":"4. Further Developments and Research Directions","text":"<ul> <li> <p>Perceptual Models:   Modern codecs increasingly integrate models of human perception (both visual and auditory) to further optimize which information can be discarded. This includes research into just-noticeable differences (JND) and masking effects.</p> </li> <li> <p>Machine Learning Approaches:   Recently, deep learning techniques have been applied to both transform coding (learning optimal transforms) and end-to-end compression systems. These methods may soon influence standards as they show promise in adapting to content dynamically.</p> </li> <li> <p>Error Resilience:   As streaming and wireless communications became prevalent, robustness against transmission errors has become more important. Techniques such as unequal error protection and adaptive streaming have been incorporated into newer codec designs.</p> </li> <li> <p>Standards Evolution:   The continuous push for higher compression efficiency without sacrificing quality drives research. Beyond HEVC, standards like Versatile Video Coding (VVC) and AV1 are being developed, incorporating many of the traditional techniques but also exploring new paradigms.</p> </li> </ul>"},{"location":"wiki/compression/lossy/lossy/#5-summary","title":"5. Summary","text":"<p>Lossy compression for images and videos has evolved through decades of research, starting from theoretical foundations in information theory to practical systems like JPEG and MPEG. Key techniques such as transform coding (DCT and DWT), quantization, entropy coding, and motion compensation have been refined over time. Historical milestones\u2014such as the standardization of JPEG in 1992, MPEG-\u00bd in the 1990s, H.264 in 2003, and HEVC in 2013\u2014reflect the growing demands for efficient multimedia storage and transmission. Modern research continues to push the boundaries by integrating perceptual models and machine learning approaches.</p> <p>This expanded overview should give you a deeper understanding of both the technical underpinnings and the historical evolution of lossy compression for image and video media.</p>"},{"location":"wiki/data-engineering/","title":"Data Engineering","text":""},{"location":"wiki/digital-signal-processing/","title":"Digital Signal Processing","text":""},{"location":"wiki/digital-signal-processing/#mpeg","title":"MPEG","text":""},{"location":"wiki/digital-signal-processing/#av1","title":"AV1","text":""},{"location":"wiki/embedded-systems/","title":"Embedded Systems","text":""},{"location":"wiki/embedded-systems/embedded-linux/","title":"Embedded Linux","text":""},{"location":"wiki/embedded-systems/embedded-linux/#hardware-emulation","title":"Hardware Emulation","text":""},{"location":"wiki/radio/ham/","title":"HAM Radio Licence","text":""},{"location":"wiki/radio/sdr/","title":"Software Defined Radio (SDR)","text":"<p>AMD PetaLinux Tools</p> <p>https://www.futuresdr.org/blog/</p> <p>https://www.bastibl.net/blog/</p>"},{"location":"blog/archive/2024/","title":"2024","text":""},{"location":"blog/archive/2020/","title":"2020","text":""},{"location":"blog/archive/2017/","title":"2017","text":""},{"location":"blog/archive/2016/","title":"2016","text":""},{"location":"blog/category/opinionated/","title":"Opinionated","text":""},{"location":"blog/category/rsw-engineering/","title":"RSW Engineering","text":""},{"location":"blog/category/tooling/","title":"Tooling","text":""},{"location":"blog/category/conferences/","title":"Conferences","text":""},{"location":"blog/category/embedded-systems/","title":"Embedded Systems","text":""},{"location":"blog/category/life/","title":"Life","text":""},{"location":"blog/category/tinyml--edge-ai/","title":"tinyML + Edge A.I.","text":""}]}